<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[minimaxir | Max Woolf's Blog]]></title>
  <link href="http://minimaxir.com/rss.xml" rel="self"/>
  <link href="http://minimaxir.com/"/>
  <updated>2017-11-07T08:15:17-08:00</updated>
  <id>http://minimaxir.com/</id>
  <author>
    <name><![CDATA[Max Woolf]]></name>
    <email><![CDATA[max@minimaxir.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Making Magic: the GIFening]]></title>
    <link href="http://minimaxir.com/2017/11/magic-the-gifening/"/>
    <updated>2017-11-07T08:10:00-08:00</updated>
    <id>http://minimaxir.com/2017/11/magic-the-gifening</id>
    <content type="html"><![CDATA[<p>After working at <a href="https://www.buzzfeed.com/">BuzzFeed</a> for a few months, I&rsquo;m now an expert in the proper usage of GIFs. My favorite GIF tool is the <a href="https://giphy.com">/giphy</a> command in <a href="https://slack.com">Slack</a>, which <a href="https://get.slack.help/hc/en-us/articles/204714258-Add-Giphy-search-to-Slack">puts a random GIF</a> according to a given phrase into the chat, with better-than-expected appropriateness of the phrase to the GIF.</p>

<p>Completely unrelated, I recently rediscovered <a href="https://github.com/Zulko/moviepy">MoviePy</a>, a Python library for programmatically editing videos and GIFs without requiring an expensive and slow video editing program. I had played with MoviePy a bit in 2014 when it was <a href="http://zulko.github.io/blog/2014/01/23/making-animated-gifs-from-video-files-with-python/#">first released</a> and <a href="https://news.ycombinator.com/item?id=7121104">became viral</a>, but couldn&rsquo;t think of a creative application for the library at the time.</p>

<p>On a boring weekend I had a silly idea: why not create a program to superimpose appropriate GIFs onto <a href="https://magic.wizards.com/en">Magic: the Gathering</a> cards using these two tools? And even better, why not <em>automate</em> both the creation of the card GIFs and the tweeting of a new GIF every few hours?</p>

<p><span><blockquote class="twitter-tweet" data-lang="en"><p lang="und" dir="ltr"><a href="https://t.co/sxqKUYHmfv">pic.twitter.com/sxqKUYHmfv</a></p>&mdash; Magic: The GIFening (@MTGIFening) <a href="https://twitter.com/MTGIFening/status/913993793052880897?ref_src=twsrc%5Etfw">September 30, 2017</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </span></p>

<p>As it turns out, creating a Twitter bot to tweet Magic card GIFs is <a href="https://github.com/minimaxir/magic-the-gifening">easy to implement</a>, but with a few interesting caveats. The end result is <a href="https://twitter.com/MTGIFening">@MTGIFening</a>. Here&rsquo;s how I typically create my crazy apps, step by step.</p>

<h2>Feasibility Analysis</h2>

<p>Like all my data analysis projects, I checked if it&rsquo;s possible to complete the project in a way that won&rsquo;t suck up a lot of free time hacking out convoluted solutions.</p>

<p><strong>Can I easily get a list of all Magic cards?</strong> Yes, via <a href="https://mtgjson.com">MTG JSON</a>, which has a downloadable JSON dump of all Magic cards.</p>

<p><strong>Can I easily get random GIFs from GIPHY?</strong> Yes, there is a /random endpoint in the <a href="https://developers.giphy.com">GIPHY API</a> which returns a random GIF for a specified phrase, like the /giphy Slack command. The GIPHY API requires registration, but has generous rate limits (10k requests/day).</p>

<p><strong>Can I easily composite a GIF onto an image with MoviePy?</strong> Yes, compositing is a <em>primary use case</em> for the library, with many tutorials in the documentation.</p>

<p><strong>Can I easily get an image for a specified Magic card?</strong> Unsure. The official tool for viewing Magic card images is <a href="http://gatherer.wizards.com/Pages/Default.aspx">Gatherer</a>. After checking the image source for the cards, each card image in Gatherer has a URL that follows this schema: <code>http://gatherer.wizards.com/Handlers/Image.ashx?multiverseid=XXXXX&amp;type=card</code>. That&rsquo;s easy to understand, but what&rsquo;s a multiverseid?</p>

<p><strong>Is there a mapping of multiverseid to Magic cards from MTG JSON?</strong> Yes, the multiverseid for each Magic card is <a href="https://mtgjson.com/documentation.html">present as a field</a> in the &ldquo;All Sets&rdquo; dataset (but not the &ldquo;All Cards&rdquo; dataset oddly). A quick manual check showed that using the multiverseid from the MTG JSON dataset results in the correct image from Gatherer.</p>

<p>Everything looked good to me. Let&rsquo;s dive right in, <a href="https://github.com/minimaxir/magic-the-gifening/commits/master">commit by commit</a>.</p>

<h2>Implementing Magic: The GIFening</h2>

<p>The first thing I did was process the Magic card data, although for this project I limit the type of cards to Instants and Sorceries, which in Magic game mechanics represent &ldquo;actions&rdquo; and are more suitable for GIFs. <em>For each set, retrieve the cards in the set; for each card, if it&rsquo;s an Instant/Sorcery, log its name and multiverseid</em>. Thanks to the magic of Python, this pseudocode is close to the <a href="https://github.com/minimaxir/magic-the-gifening/commit/3f626ae5d49a567322c6237210ab554281d462f4">actual code</a>.</p>

<p>The next objective was to implement the GIPHY API to get a GIF. The very first thing I did is add a local secrets file containing my personal API key for GIPHY, and <em>immediately</em> log the secrets file in a <code>.gitignore</code> so I don&rsquo;t accidentally leak it. GIPHY has an <a href="https://developers.giphy.com/explorer/">API Explorer</a> which allows developers to quickly test an example input phrase and see corresponding output from the API. For example, here&rsquo;s part of what the API returns for <a href="http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=151108">Invert the Skies</a> (although since it&rsquo;s the /random endpoint, your results may vary):</p>
<div class="highlight"><pre><code class="language-json" data-lang="json"><span class="s2">"image_url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://media1.giphy.com/media/plbsEwLwQvzLa/giphy.gif"</span><span class="p">,</span><span class="w">
</span><span class="s2">"image_mp4_url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://media1.giphy.com/media/plbsEwLwQvzLa/giphy.mp4"</span><span class="p">,</span><span class="w">
</span><span class="s2">"image_frames"</span><span class="p">:</span><span class="w"> </span><span class="s2">"31"</span><span class="p">,</span><span class="w">
</span><span class="s2">"image_width"</span><span class="p">:</span><span class="w"> </span><span class="s2">"480"</span><span class="p">,</span><span class="w">
</span><span class="s2">"image_height"</span><span class="p">:</span><span class="w"> </span><span class="s2">"270"</span><span class="p">,</span><span class="w">
</span></code></pre></div>
<p>The <code>image_url</code> corresponds to the <a href="https://media1.giphy.com/media/plbsEwLwQvzLa/giphy.gif">raw GIF</a> unsurprisingly, but as a bonus, GIPHY also includes a link to a <a href="https://media1.giphy.com/media/plbsEwLwQvzLa/giphy.mp4">MP4 video</a> of the GIF, which has a much smaller file size and is better to use for compositing. The API output also includes the width and height (in pixels) of the GIF. The art in a Magic card follows a 4:3 <a href="https://en.wikipedia.org/wiki/Aspect_ratio_(image)">aspect ratio</a>, i.e. the width divided by the height equals 1.33. If the dimensions of the GIF are too far outside that ratio, resizing the GIFs to fit the Magic art frame will result in noticeable distortion. I minimized this distortion by checking and seeing if the random GIF has a width:height ratio between 1.2 and 1.6 before accepting it. Since there&rsquo;s a chance for failure (along with potential unknown bugs that the random GIF could hit), I added a limit to the number of attempts to retrieve an appropriate GIF. All done in <a href="https://github.com/minimaxir/magic-the-gifening/commit/c2e4b6b9d58d1aa360f6f67a049ec962d0430b91">one commit</a>.</p>

<p>Getting the card image from Gatherer is <a href="https://github.com/minimaxir/magic-the-gifening/commit/c92440cd459640da9346cf31a79e768ac8641ea9">trivial</a>, so then I worked on combining the GIF and the card image. MoviePy has a <a href="http://zulko.github.io/moviepy/getting_started/compositing.html">good tutorial</a> for specifying the position of one clip onto another by specifying the upper-left corner of the bottom-image where the GIF will be placed, while simultaneously resizing the GIF to a given width and height.</p>

<p><img src="/img/magic-the-gifening/videoWH.jpeg" alt=""></p>

<p>I manually zoomed into the card image using a photo editor (<a href="http://www.pixelmator.com/mac/">Pixelmator</a>) to find the upper-left corner of the card art:</p>

<p><img src="/img/magic-the-gifening/zoomin.png" alt=""></p>

<p>In this case, the pixel coordinates for the upper-left corner of the card art is <code>(17,35)</code> The upper-right and bottom-left corners can be used to determine the target width and height of the GIF respectively, and can be found the same way. Simply composite the Magic card with the resized-and-positioned GIF, set the duration of the &ldquo;new&rdquo; GIF to that of the source GIF, and <code>write_gif</code>. <a href="https://github.com/minimaxir/magic-the-gifening/commit/55a52ddfc7f43d128c08c8a243254e08a171de5e">That&rsquo;s that</a>!</p>

<p>To finish things up, I wrote a script to load all the cards from the processed card list into memory, select a card at random, use the helper functions to retrieve a GIPHY GIF and composite it with the card, then upload the resulting GIF to Twitter. I haven&rsquo;t worked with the Twitter API in awhile; a quick Google search for a modern Twitter API client in Python returns <a href="https://github.com/ryanmcgrath/twython">Twython</a>, which conveniently includes an example on <a href="https://twython.readthedocs.io/en/latest/usage/advanced_usage.html#updating-status-with-image">how to upload an image to Twitter</a>! And after running the script a few times, the full workflow indeed works!</p>

<p>Not bad for a couple hours of scripting. But I was not close to finished.</p>

<h2>The Endless Fun of QA</h2>

<p>One of the reasons I enjoy doing silly projects (especially silly data projects) is because I tend to hit unsexy edge cases which typical development blogs and tutorials rarely discuss. In this case, I quickly found that the Twitter API has a <a href="https://developer.twitter.com/en/docs/media/upload-media/overview">5 MB limit</a> on image uploads, which is a problem as the resulting GIFs are huge and often randomly exceed that limit (looking back on it, there is a different endpoint intended for GIF uploads, counterintuitively).</p>

<p>In actuality, GIFs on Twitter are actually displayed as videos, in order to save bandwidth. Since Twitter transcodes uploaded GIFs anyways, it makes more sense to upload <em>audioless videos</em> instead of GIFs (and as a bonus, after the death of Vine, Twitter will auto-loop videos less than 6 seconds).</p>

<p>Creating videos is easy to do with MoviePy, just do a <code>write_videofile</code> instead of <code>write_gif</code>, and use Twython&rsquo;s video uploading example to upload. The result is an &ldquo;unknown&rdquo; error on upload. I verify by uploading the video manually to Twitter&hellip;and the Twitter UI fails to recognize it as a video. But the video itself plays fine in QuickTime. This is the annoying type of coding problem that&rsquo;s too specific for <a href="https://stackoverflow.com">Stack Overflow</a> to provide help. After a bit of trial and error involving video codecs and settings, the solution was to pass a <code>-pix_fmt yuv420p</code> parameter to the video encoder because Twitter apparently only likes legacy video container formats. Oh well. It worked, and both Twitter manual and API uploads worked successfully.</p>

<p>I also ran into an issue where Twitter refused to accept supershort video, where the source GIF was only a couple frames. A solution is to loop the GIF to atleast 2 seconds if it&rsquo;s shorter, which somehow fixed that problem.</p>

<p>(As I was writing this post a month later, I discovered that both of these video upload constraints <a href="https://developer.twitter.com/en/docs/media/upload-media/uploading-media/media-best-practices">are indeed covered in the Twitter documentation</a>, which makes me look very silly in retrospect!)</p>

<p>These changes fixed most of the upload issues. However, when writing the initial script, I forgot that the borders of Magic cards have <a href="https://mtg.gamepedia.com/Card_frame">changed over the years</a>, which also changed the position and size of the card art. <strong>Is there a way to check when a card was printed?</strong> Yes, the &ldquo;All Sets&rdquo; dataset contains the release date of the set, so with that, I can <a href="https://github.com/minimaxir/magic-the-gifening/commit/0dfb678f1955f50b54b632e57087df847ec16f05">hard code</a> the dates of sets where the borders changed, and note the border type at printing time. I then used Pixelmator again to note the new art dimensions for that type of border, and used conditional statements to retrieve the correct dimensions for the type of border when compositing.</p>

<p>Lastly, I added general <code>try/catch</code> error handling to prevent the script from breaking fatally and to try again with a different card if it does. That covers most of the edge cases!</p>

<h2>Results</h2>

<p>After running the script many times after all the fixes in place, I felt the Twitter account was good to go. The initial results showed a lot of promise:</p>

<p><span><blockquote class="twitter-tweet" data-lang="en"><p lang="und" dir="ltr"><a href="https://t.co/BsZ7eIcunl">pic.twitter.com/BsZ7eIcunl</a></p>&mdash; Magic: The GIFening (@MTGIFening) <a href="https://twitter.com/MTGIFening/status/913981726182981632?ref_src=twsrc%5Etfw">September 30, 2017</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></span></p>

<p><span><blockquote class="twitter-tweet" data-lang="en"><p lang="und" dir="ltr"><a href="https://t.co/picJJk6mBm">pic.twitter.com/picJJk6mBm</a></p>&mdash; Magic: The GIFening (@MTGIFening) <a href="https://twitter.com/MTGIFening/status/912525635632775168?ref_src=twsrc%5Etfw">September 26, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></span></p>

<p>Surprisingly, the script was able to generate <em>visual puns</em> in cards, completely by chance!</p>

<p><span><blockquote class="twitter-tweet" data-lang="en"><p lang="und" dir="ltr"><a href="https://t.co/AnpzU8xVho">pic.twitter.com/AnpzU8xVho</a></p>&mdash; Magic: The GIFening (@MTGIFening) <a href="https://twitter.com/MTGIFening/status/913972922330497024?ref_src=twsrc%5Etfw">September 30, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></span></p>

<p><span><blockquote class="twitter-tweet" data-lang="en"><p lang="und" dir="ltr"><a href="https://t.co/01vGRcq2Mj">pic.twitter.com/01vGRcq2Mj</a></p>&mdash; Magic: The GIFening (@MTGIFening) <a href="https://twitter.com/MTGIFening/status/913987002235740160?ref_src=twsrc%5Etfw">September 30, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></span></p>

<p>The next step was to automate the script to run and post Tweets at a specific time interval. After experimenting a bit, I found that <a href="https://github.com/minimaxir/magic-the-gifening/commit/f355d5e80503c67c6e1a0e5fd1b744faf3cf8223">the best solution</a> was to use a <a href="https://en.wikipedia.org/wiki/Cron">cron job</a> in a <a href="https://www.docker.com/what-docker">Docker container</a> containing the script and its dependencies, for complicated reasons which will require another blog post to explain. </p>

<p>After letting Magic: the GIFening run for a few days without fatal issues, I decided to publicize the Twitter account and posted it to the <a href="https://www.reddit.com/r/magicTCG/comments/7598g5/i_made_a_twitter_bot_which_tweets_magic_cards/">/r/MagicTCG subreddit</a> and <a href="https://news.ycombinator.com/item?id=15449955">Hacker News</a>. To my surprise, the project performed extremely well on both with 100+ upvotes on each, and the <a href="https://github.com/minimaxir/magic-the-gifening">GitHub repo</a> itself received 100+ Stars.</p>

<p>In all, making Magic: the GIFening was a fun project. In retrospect, talking though the commits made me realize I performed many bad coding practices in a haste to get the project done ASAP (specifically, checking to see if certain edge cases are documented, violating <a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a>, and forgetting to remove specific types of cards like <a href="https://twitter.com/MTGIFening/status/924969160307744769">split cards</a>). Obviously there isn&rsquo;t a multimillion-dollar startup opportunity in creating random GIFs of Magic cards, but I&rsquo;ll fix a few remaining issues and keep the Twitter bot running.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Make High Quality Data Visualizations for Websites With R and ggplot2]]></title>
    <link href="http://minimaxir.com/2017/08/ggplot2-web/"/>
    <updated>2017-08-14T09:00:00-07:00</updated>
    <id>http://minimaxir.com/2017/08/ggplot2-web</id>
    <content type="html"><![CDATA[<p>If you&rsquo;ve been following my blog, I like to use <a href="https://cran.r-project.org">R</a> and <a href="http://ggplot2.tidyverse.org/reference/">ggplot2</a> for data visualization. A lot.</p>

<p>One of my older blog posts, <a href="http://minimaxir.com/2015/02/ggplot-tutorial/">An Introduction on How to Make Beautiful Charts With R and ggplot2</a>, is still one of my most-trafficked posts years later, and even today I see techniques from that particular post incorporated into modern data visualizations on sites such as <a href="https://www.reddit.com">Reddit&rsquo;s</a> <a href="https://www.reddit.com/r/dataisbeautiful/">/r/dataisbeautiful</a> subreddit.</p>

<p>However, that post is a little outdated. Thanks to a few updates to ggplot2 since then and other advances in data visualization best-practices, making pretty charts for websites/blogs using R and ggplot2 is even more easy, quick, <em>and</em> fun!</p>

<h2>Quick Introduction to ggplot2</h2>

<p>ggplot2 uses a more concise setup toward creating charts as opposed to the more declarative style of Python&rsquo;s <a href="https://matplotlib.org">matplotlib</a> and base R. And it also includes a few example datasets for practicing ggplot2 functionality; for example, the <code>mpg</code> dataset is a <a href="http://ggplot2.tidyverse.org/reference/mpg.html">dataset</a> of the performance of popular models of cars in 1998 and 2008.</p>

<p><img src="/img/ggplot2-web/mpg.png" alt=""></p>

<p>Let&rsquo;s say you want to create a <a href="https://en.wikipedia.org/wiki/Scatter_plot">scatter plot</a>. Following <a href="http://ggplot2.tidyverse.org/reference/geom_smooth.html">a great example</a> from the ggplot2 documentation, let&rsquo;s plot the highway mileage of the car vs. the <a href="https://en.wikipedia.org/wiki/Engine_displacement">volume displacement</a> of the engine. In ggplot2, first you instantiate the chart with the <code>ggplot()</code> function, specifying the source dataset and the core aesthetics you want to plot, such as x, y, color, and fill. In this case, we set the core aesthetics to x = displacement and y = mileage, and add a <code>geom_point()</code> layer to make a scatter plot:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">mpg</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">displ</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hwy</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
            </span><span class="n">geom_point</span><span class="p">()</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/plot1.png" alt=""></p>

<p>As we can see, there is a negative correlation between the two metrics. I&rsquo;m sure you&rsquo;ve seen plots like these around the internet before. But with only a couple of lines of codes, you can make them look more contemporary.</p>

<p>ggplot2 lets you add a well-designed theme with just one line of code. Relatively new to <code>ggplot2</code> is <code>theme_minimal()</code>, which <a href="http://ggplot2.tidyverse.org/reference/ggtheme.html">generates</a> a muted style similar to <a href="http://fivethirtyeight.com">FiveThirtyEight</a>&rsquo;s modern data visualizations:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">theme_minimal</span><span class="p">()</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/plot2.png" alt=""></p>

<p>But we can still add color. Setting a color aesthetic on a character/categorical variable will set the colors of the corresponding points, making it easy to differentiate at a glance.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">mpg</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">displ</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hwy</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="o">=</span><span class="n">class</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
            </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
            </span><span class="n">theme_minimal</span><span class="p">()</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/plot3.png" alt=""></p>

<p>Adding the color aesthetic certainly makes things much prettier. ggplot2 automatically adds a legend for the colors as well. 
However, for this particular visualization, it is difficult to see trends in the points for each class. A easy way around this is to add a <a href="https://en.wikipedia.org/wiki/Least_squares">least squares regression</a> trendline for each class <a href="http://ggplot2.tidyverse.org/reference/geom_smooth.html">using</a> <code>geom_smooth()</code> (which normally adds a smoothed line, but since there isn&rsquo;t a lot of data for each group, we force it to a linear model and do not plot confidence intervals)</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lm"</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">F</span><span class="p">)</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/plot4.png" alt=""></p>

<p>Pretty neat, and now comparative trends are much more apparent! For example, pickups and SUVs have similar efficiency, which makes intuitive sense.</p>

<p>The chart axes should be labeled (<em>always</em> label your charts!). All the typical labels, like <code>title</code>, <code>x</code>-axis, and <code>y</code>-axis can be done with the <code>labs()</code> function. But relatively new to ggplot2 are the <code>subtitle</code> and <code>caption</code> fields, both of do what you expect:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">labs</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Efficiency of Popular Models of Cars"</span><span class="p">,</span><span class="w">
         </span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"By Class of Car"</span><span class="p">,</span><span class="w">
         </span><span class="n">x</span><span class="o">=</span><span class="s2">"Engine Displacement (liters)"</span><span class="p">,</span><span class="w">
         </span><span class="n">y</span><span class="o">=</span><span class="s2">"Highway Miles per Gallon"</span><span class="p">,</span><span class="w">
         </span><span class="n">caption</span><span class="o">=</span><span class="s2">"by Max Woolf — minimaxir.com"</span><span class="p">)</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/plot5.png" alt=""></p>

<p>That&rsquo;s a pretty good start. Now let&rsquo;s take it to the next level.</p>

<h2>How to Save A ggplot2 chart For Web</h2>

<p>Something surprisingly undiscussed in the field of data visualization is how to <em>save</em> a chart as a high quality image file. For example, with <a href="https://products.office.com/en-us/excel">Excel</a> charts, Microsoft <a href="https://support.office.com/en-us/article/Save-a-chart-as-a-picture-in-Excel-for-Windows-254bbf9a-1ce1-459f-914a-4902e8ca9217">officially recommends</a> to copy the chart, <em>paste it as an image back into Excel</em>, then save the pasted image, without having any control over image quality and size in the browser (the <em>real</em> best way to save an Excel/<a href="https://www.apple.com/numbers/">Numbers</a> chart as an image for a webpage is to copy/paste the chart object into a <a href="https://products.office.com/en-us/powerpoint">PowerPoint</a>/<a href="https://www.apple.com/keynote/">Keynote</a> slide, and export <em>the slide</em> as an image. This also makes it extremely easy to annotate/brand said chart beforehand in PowerPoint/Keynote).</p>

<p>R IDEs such as <a href="https://www.rstudio.com">RStudio</a> have a chart-saving UI with the typical size/filetype options. But if you save an image from this UI, the shapes and texts of the resulting image will be heavily aliased (R <a href="https://danieljhocking.wordpress.com/2013/03/12/high-resolution-figures-in-r/">renders images at 72 dpi</a> by default, which is much lower than that of modern HiDPI/Retina displays).</p>

<p>The data visualizations used earlier in this post were generated in-line as a part of an <a href="http://rmarkdown.rstudio.com/r_notebooks.html">R Notebook</a>, but it is surprisingly difficult to extract the generated chart as a separate file. But ggplot2 also has <code>ggsave()</code>, which saves the image to disk using antialiasing and makes the fonts/shapes in the chart look much better, and assumes a default dpi of 300. Saving charts using <code>ggsave()</code>, and adjusting the sizes of the text and geoms to compensate for the higher dpi, makes the charts look very presentable. A width of 4 and a height of 3 results in a 1200x900px image, which if posted on a blog with a content width of ~600px (like mine), will render at full resolution on HiDPI/Retina displays, or downsample appropriately otherwise. Due to modern PNG compression, the file size/bandwidth cost for using larger images is minimal.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">mpg</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">displ</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hwy</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="o">=</span><span class="n">class</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lm"</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="o">=</span><span class="nb">F</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_point</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">theme_minimal</span><span class="p">(</span><span class="n">base_size</span><span class="o">=</span><span class="m">9</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">labs</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Efficiency of Popular Models of Cars"</span><span class="p">,</span><span class="w">
         </span><span class="n">subtitle</span><span class="o">=</span><span class="s2">"By Class of Car"</span><span class="p">,</span><span class="w">
         </span><span class="n">x</span><span class="o">=</span><span class="s2">"Engine Displacement (liters)"</span><span class="p">,</span><span class="w">
         </span><span class="n">y</span><span class="o">=</span><span class="s2">"Highway Miles per Gallon"</span><span class="p">,</span><span class="w">
         </span><span class="n">caption</span><span class="o">=</span><span class="s2">"by Max Woolf — minimaxir.com"</span><span class="p">)</span><span class="w">

</span><span class="n">ggsave</span><span class="p">(</span><span class="s2">"tutorial-0.png"</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/tutorial-0.png" alt=""></p>

<p>Compare to the previous non-ggsave chart, which is more blurry around text/shapes:</p>

<p><img src="/img/ggplot2-web/plot5.png" alt=""></p>

<p>For posterity, here&rsquo;s the same chart saved at 1200x900px using the RStudio image-saving UI:</p>

<p><img src="/img/ggplot2-web/plot-1200-900.png" alt=""></p>

<p>Note that the antialiasing optimizations assume that you are <em>not</em> uploading the final chart to a service like <a href="https://medium.com">Medium</a> or <a href="https://wordpress.com">WordPress.com</a>, which will compress the images and reduce the quality anyways. But if you are uploading it to Reddit or self-hosting your own blog, it&rsquo;s definitely worth it.</p>

<h2>Fancy Fonts</h2>

<p>Changing the chart font is another way to add a personal flair.
Theme functions like <code>theme_minimal()</code> accept a <code>base_family</code> parameter. With that, you can specify any font family as the default instead of the base sans-serif. (On Windows, you may need to install the <code>extrafont</code> package first). Fonts from <a href="https://fonts.google.com">Google Fonts</a> are free and work easily with ggplot2 once installed. For example, we can use <a href="https://fonts.google.com/specimen/Roboto">Roboto</a>, Google&rsquo;s modern font which has also been getting a lot of usage on <a href="https://stackoverflow.com">Stack Overflow</a>&rsquo;s great ggplot2 <a href="https://stackoverflow.blog/2017/06/15/developers-use-spaces-make-money-use-tabs/">data visualizations</a>.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">theme_minimal</span><span class="p">(</span><span class="n">base_size</span><span class="o">=</span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">base_family</span><span class="o">=</span><span class="s2">"Roboto"</span><span class="p">)</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/tutorial-1.png" alt=""></p>

<p>A general text design guideline is to use fonts of different weights/widths for different hierarchies of content. In this case, we can use a bolder condensed font for the title, and deemphasize the subtitle and caption using lighter colors, all done using the <code>theme()</code> <a href="http://ggplot2.tidyverse.org/reference/theme.html">function</a>.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">theme</span><span class="p">(</span><span class="n">plot.subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">"#666666"</span><span class="p">),</span><span class="w">
          </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">family</span><span class="o">=</span><span class="s2">"Roboto Condensed Bold"</span><span class="p">),</span><span class="w">
          </span><span class="n">plot.caption</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">"#AAAAAA"</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">6</span><span class="p">))</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/tutorial-2.png" alt=""></p>

<p>It&rsquo;s worth nothing that data visualizations posted on websites should be easily <em>legible</em> for mobile-device users as well, hence the intentional use of larger fonts relative to charts typically produced in the desktop-oriented Excel.</p>

<p>Additionally, all theming options can be set as a session default at the beginning of a script using <code>theme_set()</code>, saving even more time instead of having to recreate the theme for each chart.</p>

<h2>The &ldquo;ggplot2 colors&rdquo;</h2>

<p>The &ldquo;ggplot2 colors&rdquo; for categorical variables are infamous for being the primary indicator of a chart being made with ggplot2. But there is a science to it; ggplot2 by default selects colors using the <code>scale_color_hue()</code> <a href="http://ggplot2.tidyverse.org/reference/scale_hue.html">function</a>, which selects colors in the HSL space by changing the hue [H] between 0 and 360, keeping saturation [S] and lightness [L] constant. As a result, ggplot2 selects the most <em>distinct</em> colors possible while keeping lightness constant. For example, if you have 2 different categories, ggplot2 chooses the colors with h = 0 and h = 180; if 3 colors, h = 0, h = 120, h = 240, etc.</p>

<p>It&rsquo;s smart, but does make a given chart lose distinctness when many other ggplot2 charts use the same selection methodology. A quick way to take advantage of this hue dispersion while still making the colors unique is to change the lightness; by default, <code>l = 65</code>, but setting it slightly lower will make the charts look more professional/<a href="https://www.bloomberg.com">Bloomberg</a>-esque.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p_color</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w">
        </span><span class="n">scale_color_hue</span><span class="p">(</span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">40</span><span class="p">)</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/tutorial-4.png" alt=""></p>

<h2>RColorBrewer</h2>

<p>Another coloring option for ggplot2 charts are the <a href="http://colorbrewer2.org/#type=sequential&amp;scheme=BuGn&amp;n=3">ColorBrewer</a> palettes implemented with the <code>RColorBrewer</code> package, which are supported natively in ggplot2 with functions such as <code>scale_color_brewer()</code>. The sequential palettes like &ldquo;Blues&rdquo; and &ldquo;Greens&rdquo; do what the name implies:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p_color</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w">
        </span><span class="n">scale_color_brewer</span><span class="p">(</span><span class="n">palette</span><span class="o">=</span><span class="s2">"Blues"</span><span class="p">)</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/tutorial-5.png" alt=""></p>

<p>A famous diverging palette for visualizations on /r/dataisbeautiful is the &ldquo;Spectral&rdquo; palette, which is a lighter rainbow (recommended for dark backgrounds)</p>

<p><img src="/img/ggplot2-web/tutorial-6.png" alt=""></p>

<p>However, while the charts look pretty, it&rsquo;s difficult to tell the categories apart. The qualitative palettes fix this problem, and have more distinct possibilities than the <code>scale_color_hue()</code> approach mentioned earlier.</p>

<p>Here are 3 examples of qualitative palettes, &ldquo;Set1&rdquo;, &ldquo;Set2&rdquo;, and &ldquo;Set3,&rdquo; whichever fit your preference.</p>

<p><img src="/img/ggplot2-web/tutorial-7.png" alt="">
<img src="/img/ggplot2-web/tutorial-8.png" alt="">
<img src="/img/ggplot2-web/tutorial-9.png" alt=""></p>

<h2>Viridis and Accessibility</h2>

<p>Let&rsquo;s mix up the visualization a bit. A rarely-used-but-very-useful ggplot2 geom is <code>geom2d_bin()</code>, which counts the number of points in a given 2d spatial area:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">mpg</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">displ</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hwy</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">geom_bin2d</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="p">[</span><span class="n">...theming</span><span class="w"> </span><span class="n">options...</span><span class="p">]</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/tutorial-tile.png" alt=""></p>

<p>We see that the largest number of points are centered around (2,30). However, the default ggplot2 color palette for continuous variables is <em>boring</em>. Yes, we can use the RColorBrewer sequential palettes above, but as noted, they aren&rsquo;t perceptually distinct, and could cause issues for readers who are colorblind.</p>

<p>The <a href="https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html">viridis R package</a> provides a set of 4 high-contrast palettes which are very colorblind friendly, and works easily with ggplot2 by extending a <code>scale_fill_viridis()/scale_color_viridis()</code> function.</p>

<p>The default &ldquo;viridis&rdquo; palette has been increasingly popular on the web lately:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">p_color</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w">
        </span><span class="n">scale_fill_viridis</span><span class="p">(</span><span class="n">option</span><span class="o">=</span><span class="s2">"viridis"</span><span class="p">)</span><span class="w">
</span></code></pre></div>
<p><img src="/img/ggplot2-web/tutorial-10.png" alt=""></p>

<p>&ldquo;magma&rdquo; and &ldquo;inferno&rdquo; are similar, and give the data visualization a fiery edge:</p>

<p><img src="/img/ggplot2-web/tutorial-11.png" alt=""></p>

<p><img src="/img/ggplot2-web/tutorial-12.png" alt=""></p>

<p>Lastly, &ldquo;plasma&rdquo; is a mix between the 3 palettes above:</p>

<p><img src="/img/ggplot2-web/tutorial-13.png" alt=""></p>

<h2>Next Steps</h2>

<p>FiveThirtyEight actually uses ggplot2 for their data journalism workflow <a href="https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/FiveThirtyEights-data-journalism-workflow-with-R?ocid=player">in an interesting way</a>; they render the base chart using ggplot2, but export it as as a SVG/PDF vector file which can scale to any size, and then the design team annotates/customizes the data visualization in <a href="http://www.adobe.com/products/illustrator.html">Adobe Illustrator</a> before exporting it as a static PNG for the article (in general, I recommend using an external image editor to add text annotations to a data visualization because doing it manually in ggplot2 is inefficient).</p>

<p>For general use cases, ggplot2 has very strong defaults for beautiful data visualizations. And certainly there is a lot <em>more</em> you can do to make a visualization beautiful than what&rsquo;s listed in this post, such as using facets and tweaking parameters of geoms for further distinction, but those are more specific to a given data visualization. In general, it takes little additional effort to make something <em>unique</em> with ggplot2, and the effort is well worth it. And prettier charts are more persuasive, which is a good return-on-investment.</p>

<hr>

<p><em>You can view the R and ggplot2 code used to create the data visualizations in <a href="http://minimaxir.com/notebooks/ggplot2-web/">this R Notebook</a>. You can also view the images/data used for this post in <a href="https://github.com/minimaxir/ggplot2-web">this GitHub repository</a></em>.</p>

<p><em>You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs]]></title>
    <link href="http://minimaxir.com/2017/07/cpu-or-gpu/"/>
    <updated>2017-07-05T09:00:00-07:00</updated>
    <id>http://minimaxir.com/2017/07/cpu-or-gpu</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been working on a few personal deep learning projects with <a href="https://github.com/fchollet/keras">Keras</a> and <a href="https://www.tensorflow.org">TensorFlow</a>. However, training models for deep learning with cloud services such as <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> and <a href="https://cloud.google.com/compute/">Google Compute Engine</a> isn&rsquo;t free, and as someone who is currently unemployed, I have to keep an eye on extraneous spending and be as cost-efficient as possible (please support my work on <a href="https://www.patreon.com/minimaxir">Patreon</a>!). I tried deep learning on the cheaper CPU instances instead of GPU instances to save money, and to my surprise, my model training was only slightly slower. As a result, I took a deeper look at the pricing mechanisms of these two types of instances to see if CPUs are more useful for my needs.</p>

<p>The <a href="https://cloud.google.com/compute/pricing#gpus">pricing of GPU instances</a> on Google Compute Engine starts at <strong>$0.745/hr</strong> (by attaching a $0.700/hr GPU die to a $0.045/hr n1-standard-1 instance). A couple months ago, Google <a href="https://cloudplatform.googleblog.com/2017/05/Compute-Engine-machine-types-with-up-to-64-vCPUs-now-ready-for-your-production-workloads.html">announced</a> CPU instances with up to 64 vCPUs on the modern Intel <a href="https://en.wikipedia.org/wiki/Skylake_(microarchitecture)">Skylake</a> CPU architecture. More importantly, they can also be used in <a href="https://cloud.google.com/compute/docs/instances/preemptible">preemptible CPU instances</a>, which live at most for 24 hours on GCE and can be terminated at any time (very rarely), but cost about <em>20%</em> of the price of a standard instance. A preemptible n1-highcpu-64 instance with 64 vCPUs and 57.6GB RAM plus the premium for using Skylake CPUs is <strong>$0.509/hr</strong>, about 2/3rds of the cost of the GPU instance.</p>

<p>If the model training speed of 64 vCPUs is comparable to that of a GPU (or even slightly slower), it would be more cost-effective to use the CPUs instead. But that&rsquo;s assuming the deep learning software and the GCE platform hardware operate at 100% efficiency; if they don&rsquo;t (and they likely don&rsquo;t), there may be <em>even more savings</em> by scaling down the number of vCPUs and cost accordingly (a 32 vCPU instance with same parameters is half the price at <strong>$0.254/hr</strong>, 16 vCPU at <strong>$0.127/hr</strong>, etc).</p>

<p>There aren&rsquo;t any benchmarks for deep learning libraries with tons and tons of CPUs since there&rsquo;s no demand, as GPUs are the <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam&rsquo;s razor</a> solution to deep learning hardware. But what might make counterintuitive but economical sense is to use CPUs instead of GPUs for deep learning training because of the massive cost differential afforded by preemptible instances, thanks to Google&rsquo;s <a href="https://en.wikipedia.org/wiki/Economies_of_scale">economies of scale</a>.</p>

<h2>Setup</h2>

<p>I already have <a href="https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark">benchmarking scripts</a> of real-world deep learning use cases, <a href="https://github.com/minimaxir/keras-cntk-docker">Docker container environments</a>, and results logging from my <a href="http://minimaxir.com/2017/06/keras-cntk/">TensorFlow vs. CNTK article</a>. A few minor tweaks allow the scripts to be utilized for both CPU and GPU instances by setting CLI arguments. I also rebuilt <a href="https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile">the Docker container</a> to support the latest version of TensorFlow (1.2.1), and created a <a href="https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu">CPU version</a> of the container which installs the CPU-appropriate TensorFlow library instead.</p>

<p>There is a notable CPU-specific TensorFlow behavior; if you install from <code>pip</code> (as the<a href="https://www.tensorflow.org/install/"> official instructions</a> and tutorials recommend) and begin training a model in TensorFlow, you&rsquo;ll see these warnings in the console:</p>

<p><img src="/img/cpu-or-gpu/tensorflow-console.png" alt=""></p>

<p>In order to fix the warnings and benefit from these <a href="https://en.wikipedia.org/wiki/SSE4#SSE4.2">SSE4.2</a>/<a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a>/<a href="https://en.wikipedia.org/wiki/FMA_instruction_set">FMA</a> optimizations, we <a href="https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions">compile TensorFlow from source</a>, and I created a <a href="https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu-compiled">third Docker container</a> to do just that. When training models in the new container, <a href="https://github.com/tensorflow/tensorflow/issues/10689">most</a> of the warnings no longer show, and (spoiler alert) there is indeed a speed boost in training time.</p>

<p>Therefore, we can test three major cases with Google Compute Engine:</p>

<ul>
<li>A Tesla K80 GPU instance.</li>
<li>A 64 Skylake vCPU instance where TensorFlow is installed via <code>pip</code> (along with testings at 8/16/32 vCPUs).</li>
<li>A 64 Skylake vCPU instance where TensorFlow is compiled (<code>cmp</code>) with CPU instructions (+ 8/16/32 vCPUs).</li>
</ul>

<h2>Results</h2>

<p>For each model architecture and software/hardware configuration, I calculate the <strong>total training time relative to the GPU instance training</strong> for running the model training for the provided test script. In all cases, the GPU <em>should</em> be the fastest training configuration, and systems with more processors should train faster than those with fewer processors.</p>

<p>Let&rsquo;s start using the <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> of handwritten digits plus the common multilayer perceptron (MLP) architecture, with dense fully-connected layers. Lower training time is better. All configurations below the horizontal dotted line are better than GPUs; all configurations above the dotted line are worse than GPUs.</p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-5.png" alt=""></p>

<p>Here, the GPU is the fastest out of all the platform configurations, but there are other curious trends: the performance between 32 vCPUs and 64 vCPUs is similar, and the compiled TensorFlow library is indeed a significant improvement in training speed <em>but only for 8 and 16 vCPUs</em>. Perhaps there are overheads negotiating information between vCPUs that eliminate the performance advantages of more vCPUs, and perhaps these overheads are <em>different</em> with the CPU instructions of the compiled TensorFlow. In the end, it&rsquo;s a <a href="https://en.wikipedia.org/wiki/Black_box">black box</a>, which is why I prefer black box benchmarking all configurations of hardware instead of theorycrafting.</p>

<p>Since the difference between training speeds of different vCPU counts is minimal, there is definitely an advantage by scaling down. For each model architecture and configuration, I calculate a <strong>normalized training cost relative to the cost of GPU instance training</strong>. Because GCE instance costs are prorated (unlike Amazon EC2), we can simply calculate experiment cost by multiplying the total number of seconds the experiment runs by the cost of the instance (per second). Ideally, we want to <em>minimize</em> cost.</p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-6.png" alt=""></p>

<p>Lower CPU counts are <em>much</em> more cost-effective for this problem, when going as low as possible is better.</p>

<p>Now, let&rsquo;s look at the same dataset with a convolutional neural network (CNN) approach for digit classification:</p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-7.png" alt=""></p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-8.png" alt=""></p>

<p>GPUs are unsurprisingly more than twice as fast as any CPU approach at CNNs, but cost structures are still the same, except that 64 vCPUs are <em>worse</em> than GPUs cost-wise, with 32 vCPUs training even faster than with 64 vCPUs.</p>

<p>Let&rsquo;s go deeper with CNNs and look at the <a href="https://www.cs.toronto.edu/%7Ekriz/cifar.html">CIFAR-10</a> image classification dataset, and a model which utilizes a deep covnet + a multilayer perceptron and ideal for image classification (similar to the <a href="https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3">VGG-16</a> architecture).</p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-9.png" alt=""></p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-10.png" alt=""></p>

<p>Similar behaviors as in the simple CNN case, although in this instance all CPUs perform better with the compiled TensorFlow library.</p>

<p>The fasttext algorithm, used here on the <a href="http://ai.stanford.edu/%7Eamaas/data/sentiment/">IMDb reviews dataset</a> to determine whether a review is positive or negative, classifies text extremely quickly relative to other methods. </p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-3.png" alt=""></p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-4.png" alt=""></p>

<p>In this case, GPUs are much, much faster than CPUs. The benefit of lower numbers of CPU isn&rsquo;t as dramatic; although as an aside, the <a href="https://github.com/facebookresearch/fastText">official fasttext implementation</a> is <em>designed</em> for large amounts of CPUs and handles parallelization much better.</p>

<p>The Bidirectional long-short-term memory (LSTM) architecture is great for working with text data like IMDb reviews, but after my previous benchmark article, <a href="https://news.ycombinator.com/item?id=14538086">commenters on Hacker News</a> noted that TensorFlow uses an inefficient implementation of the LSTM on the GPU, so perhaps the difference will be more notable.</p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-1.png" alt=""></p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-2.png" alt=""></p>

<p>Wait, what? GPU training of Bidirectional LSTMs is <em>twice as slow</em> as any CPU configuration? Wow. (In fairness, the benchmark uses the Keras LSTM default of <code>implementation=0</code> which is better on CPUs while <code>implementation=2</code> is better on GPUs, but it shouldn&rsquo;t result in that much of a differential)</p>

<p>Lastly, LSTM text generation of <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche">Nietzsche&rsquo;s</a> <a href="https://s3.amazonaws.com/text-datasets/nietzsche.txt">writings</a> follows similar patterns to the other architectures, but without the drastic hit to the GPU.</p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-11.png" alt=""></p>

<p><img src="/img/cpu-or-gpu/dl-cpu-gpu-12.png" alt=""></p>

<h2>Conclusion</h2>

<p>As it turns out, using 64 vCPUs is <em>bad</em> for deep learning as current software/hardware architectures can&rsquo;t fully utilize all of them, and it often results in the exact same performance (or <em>worse</em>) than with 32 vCPUs. In terms balancing both training speed and cost, training models with <strong>16 vCPUs + compiled TensorFlow</strong> seems like the winner. The 30%-40% speed boost of the compiled TensorFlow library was an unexpected surprise, and I&rsquo;m shocked Google doesn&rsquo;t offer a precompiled version of TensorFlow with these CPU speedups since the gains are nontrivial.</p>

<p>It&rsquo;s worth nothing that the cost advantages shown here are <em>only</em> possible with preemptible instances; regular high-CPU instances on Google Compute Engine are about 5x as expensive, and as a result eliminate the cost benefits completely. Hooray for economies of scale!</p>

<p>A major implicit assumption with the cloud CPU training approach is that you don&rsquo;t need a trained model ASAP. In professional use cases, time may be too valuable to waste, but in personal use cases where someone can just leave a model training overnight, it&rsquo;s a very, very good and cost-effective option, and one that I&rsquo;ll now utilize.</p>

<hr>

<p><em>All scripts for running the benchmark are available in <a href="https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark">this GitHub repo</a>. You can view the R/ggplot2 code used to process the logs and create the visualizations in <a href="http://minimaxir.com/notebooks/deep-learning-cpu-gpu/">this R Notebook</a>.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Predicting the Success of a Reddit Submission with Deep Learning and Keras]]></title>
    <link href="http://minimaxir.com/2017/06/reddit-deep-learning/"/>
    <updated>2017-06-26T09:00:00-07:00</updated>
    <id>http://minimaxir.com/2017/06/reddit-deep-learning</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been trying to figure out what makes a <a href="https://www.reddit.com">Reddit</a> submission &ldquo;good&rdquo; for years. If we assume the number of upvotes on a submission is a fair proxy for submission quality, optimizing a statistical model for Reddit data with submission score as a response variable might lead to interesting (and profitable) insights when transferred into other domains, such as Facebook Likes and Twitter Favorites.</p>

<p><img src="/img/reddit-deep-learning/reddit-example.png" alt=""></p>

<p>An important part of a Reddit submission is the submission <strong>title</strong>. Like news headlines, a catchy title will make a user <a href="http://minimaxir.com/2015/10/reddit-topwords/">more inclined</a> to engage with a submission and potentially upvote.</p>

<p><img src="/img/reddit-topwords/mean-054-Fitness.png" alt=""></p>

<p>Additionally, the <strong>time when the submission is made</strong> is <a href="http://minimaxir.com/2015/10/reddit-bigquery/">important</a>; submitting when user activity is the highest tends to lead to better results if you are trying to maximize exposure.</p>

<p><img src="/img/reddit-bigquery/reddit-bigquery-2.png" alt=""></p>

<p>The actual <strong>content</strong> of the Reddit submission such as  images/links to a website is likewise important, but good content is relatively difficult to optimize.</p>

<p>Can the magic of deep learning reconcile these concepts and create a model which can predict if a submission is a good submission? Thanks to <a href="https://github.com/fchollet/keras">Keras</a>, performing deep learning on a very large number of Reddit submissions is actually pretty easy. Performing it <em>well</em> is a different story.</p>

<h2>Getting the Data + Feature Engineering</h2>

<p>It&rsquo;s difficult to retrieve the content of millions of Reddit submissions at scale (ethically), so let&rsquo;s initially start by building a model using submissions on <a href="https://www.reddit.com/r/AskReddit/">/r/AskReddit</a>: Reddit&rsquo;s largest subreddit which receives 8,000+ submissions each day. /r/AskReddit is a self-post only subreddit with no external links, allowing us to focus on only the submission title and timing.</p>

<p><a href="http://minimaxir.com/2015/10/reddit-bigquery/">As always</a>, we can collect large amounts of Reddit data from the public Reddit dataset on <a href="https://cloud.google.com/bigquery/">BigQuery</a>. The submission <code>title</code> is available by default. The raw timestamp of the submission is also present, allowing us to extract the <code>hour</code> of submission (adjusted to Eastern Standard Time) and <code>dayofweek</code>, as used in the heatmap above. But why stop there? Since /r/AskReddit receives hundreds of submissions <em>every hour</em> on average, we should look at the <code>minute</code> level to see if there are any deeper trends (e.g. there are only 30 slots available on the first page of /new and since there is so much submission activity, it might be more advantageous to submit during off-peak times). Lastly, to account for potential changes in behavior as the year progresses, we should add a <code>dayofyear</code> feature, where January 1st = 1, January 2nd = 2, etc which can also account for variance due to atypical days like holidays.</p>

<p>Instead of predicting the raw number on upvotes of the Reddit submission (as the distribution of submission scores is heavily skewed), we should predict <strong>whether or not the submission is good</strong>, shaping the problem as a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>. In this case, let&rsquo;s define a &ldquo;good submission&rdquo; as one whose score is equal to or above the <strong>50th percentile (median) of all submissions</strong> in /r/AskReddit. Unfortunately, the median score ends up being <strong>2 points</strong>; although &ldquo;one upvote&rdquo; might be a low threshold for a &ldquo;good&rdquo; submission, it splits the dataset into 64% bad submissions, 36% good submissions, and setting the percentile threshold higher will result in a very unbalanced dataset for model training (a score of 2+ also implies that the submission did not get downvoted to death, which is useful). </p>

<p>Gathering all <strong>976,538 /r/AskReddit submissions</strong> from January 2017 to April 2017 should be enough data for this project. Here&rsquo;s the final BigQuery:</p>
<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="o">#</span><span class="n">standardSQL</span> 
<span class="k">SELECT</span> <span class="n">id</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span>
  <span class="k">CAST</span><span class="p">(</span><span class="n">FORMAT_TIMESTAMP</span><span class="p">(</span><span class="s1">'%H'</span><span class="p">,</span> <span class="n">TIMESTAMP_SECONDS</span><span class="p">(</span><span class="n">created_utc</span><span class="p">),</span> <span class="s1">'America/New_York'</span><span class="p">)</span> <span class="k">AS</span> <span class="n">INT64</span><span class="p">)</span> <span class="k">AS</span> <span class="n">hour</span><span class="p">,</span>
  <span class="k">CAST</span><span class="p">(</span><span class="n">FORMAT_TIMESTAMP</span><span class="p">(</span><span class="s1">'%M'</span><span class="p">,</span> <span class="n">TIMESTAMP_SECONDS</span><span class="p">(</span><span class="n">created_utc</span><span class="p">),</span> <span class="s1">'America/New_York'</span><span class="p">)</span> <span class="k">AS</span> <span class="n">INT64</span><span class="p">)</span> <span class="k">AS</span> <span class="k">minute</span><span class="p">,</span>
  <span class="k">CAST</span><span class="p">(</span><span class="n">FORMAT_TIMESTAMP</span><span class="p">(</span><span class="s1">'%w'</span><span class="p">,</span> <span class="n">TIMESTAMP_SECONDS</span><span class="p">(</span><span class="n">created_utc</span><span class="p">),</span> <span class="s1">'America/New_York'</span><span class="p">)</span> <span class="k">AS</span> <span class="n">INT64</span><span class="p">)</span> <span class="k">AS</span> <span class="n">dayofweek</span><span class="p">,</span>
  <span class="k">CAST</span><span class="p">(</span><span class="n">FORMAT_TIMESTAMP</span><span class="p">(</span><span class="s1">'%j'</span><span class="p">,</span> <span class="n">TIMESTAMP_SECONDS</span><span class="p">(</span><span class="n">created_utc</span><span class="p">),</span> <span class="s1">'America/New_York'</span><span class="p">)</span> <span class="k">AS</span> <span class="n">INT64</span><span class="p">)</span> <span class="k">AS</span> <span class="n">dayofyear</span><span class="p">,</span>
  <span class="n">IF</span><span class="p">(</span><span class="n">PERCENT_RANK</span><span class="p">()</span> <span class="n">OVER</span> <span class="p">(</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="n">score</span> <span class="k">ASC</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">as</span> <span class="n">is_top_submission</span>
  <span class="k">FROM</span> <span class="nv">`fh-bigquery.reddit_posts.*`</span>
  <span class="k">WHERE</span> <span class="p">(</span><span class="n">_TABLE_SUFFIX</span> <span class="k">BETWEEN</span> <span class="s1">'2017_01'</span> <span class="k">AND</span> <span class="s1">'2017_04'</span><span class="p">)</span>
  <span class="k">AND</span> <span class="n">subreddit</span> <span class="o">=</span> <span class="s1">'AskReddit'</span>
</code></pre></div>
<p><img src="/img/reddit-deep-learning/bigquery.png" alt=""></p>

<h2>Model Architecture</h2>

<p><em>If you want to see the detailed data transformations and Keras code examples/outputs for this post, you can view <a href="https://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb">this Jupyter Notebook</a>.</em></p>

<p>Text processing is a good use case for deep learning, as it can identify relationships between words where older methods like <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> can&rsquo;t. Keras, a high level deep-learning framework on top of lower frameworks like <a href="https://www.tensorflow.org">TensorFlow</a>, can easily convert a list of texts to a <a href="https://keras.io/preprocessing/sequence/">padded sequence</a> of <a href="https://keras.io/preprocessing/text/">index tokens</a> that can interact with deep learning models, along with many other benefits. Data scientists often use <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> that can &ldquo;learn&rdquo; for classifying text. However <a href="https://github.com/facebookresearch/fastText">fasttext</a>, a newer algorithm from researchers at Facebook, can perform classification tasks at an <a href="http://minimaxir.com/2017/06/keras-cntk/">order of magnitude faster</a> training time than RNNs, with similar predictive performance.</p>

<p>fasttext works by <a href="https://arxiv.org/abs/1607.01759">averaging word vectors</a>. In this Reddit model architecture inspired by the <a href="https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py">official Keras fasttext example</a>, each word in a Reddit submission title (up to 20) is mapped to a 50-dimensional vector from an Embeddings layer of up to 40,000 words. The Embeddings layer is <a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">initialized</a> with <a href="https://nlp.stanford.edu/projects/glove/">GloVe word embeddings</a> pre-trained on billions of words to give the model a good start. All the word vectors for a given Reddit submission title are averaged together, and then a Dense fully-connected layer outputs a probability the given text is a good submission. The gradients then backpropagate and improve the word embeddings for future batches during training.</p>

<p>Keras has a <a href="https://keras.io/visualization/">convenient utility</a> to visualize deep learning models:</p>

<p><img src="/img/reddit-deep-learning/model_shapes-1.png" alt=""></p>

<p>However, the first output above is the <em>auxiliary output</em> for <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizing</a> the word embeddings; we still have to incorporate the submission timing data into the model.</p>

<p>Each of the four timing features (hour, minute, day of week, day of year) receives its own Embeddings layer, outputting a 64D vector. This allows the features to learn latent characteristics which may be missed using traditional <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">one-hot encoding</a> for categorical data in machine learning problems.</p>

<p><img src="/img/reddit-deep-learning/model_shapes-2.png" alt=""></p>

<p>The 50D word average vector is concatenated with the four vectors above, resulting in a 306D vector. This combined vector is connected to another fully-connected layer which can account for hidden interactions between all five input features (plus <a href="https://keras.io/layers/normalization/">batch normalization</a>, which improves training speed for Dense layers). Then the model outputs a final probability prediction: the <em>main output</em>.</p>

<p><img src="/img/reddit-deep-learning/model_shapes-3.png" alt=""></p>

<p>The final model:</p>

<p><img src="/img/reddit-deep-learning/model.png" alt=""></p>

<p>All of this sounds difficult to implement, but Keras&rsquo;s <a href="https://keras.io/getting-started/functional-api-guide/">functional API</a> ensures that adding each layer and linking them together can be done in a single line of code each.</p>

<h2>Training Results</h2>

<p>Because the model uses no recurrent layers, it trains fast enough on a CPU despite the large dataset size.</p>

<p>We split the full dataset into 80%/20% training/test datasets, training the model on the former and testing the model against the latter. Keras trains a model with a simple <code>fit</code> command and trains for 20 epochs, where one epoch represents an entire pass of the training set.</p>

<p><img src="/img/reddit-deep-learning/fit.png" alt=""></p>

<p>There&rsquo;s a lot happening in the console output due to the architecture, but the main metrics of interest are the <code>main_out_acc</code>, the accuracy of the training set through the main output, and <code>val_main_out_acc</code>, the accuracy of the test set. Ideally, the accuracy of both should increase as training progresses. However, the test accuracy <em>must</em> be better than the 64% baseline (if we just say all /r/AskReddit submissions are bad), otherwise this model is unhelpful.</p>

<p>Keras&rsquo;s <a href="https://keras.io/callbacks/#csvlogger">CSVLogger</a> trivially logs all these metrics to a CSV file. Plotting the results of the 20 epochs:</p>

<p><img src="/img/reddit-deep-learning/predict-reddit-1.png" alt=""></p>

<p>The test accuracy does indeed beat the 64% baseline; however, test accuracy <em>decreases</em> as training progresses. This is a sign of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>, possibly due to the potential disparity between texts in the training and test sets. In deep learning, you can account for overfitting by adding <a href="https://keras.io/layers/core/#dropout">Dropout</a> to relevant layers, but in my testing it did not help.</p>

<h2>Using The Model To Optimize Reddit Submissions</h2>

<p>At the least, we now have a model that understands the latent characteristics of an /r/AskReddit submission. But how do you apply the model <em>in practical, real-world situations</em>?</p>

<p>Let&rsquo;s take a random /r/AskReddit submission: <a href="https://www.reddit.com/r/AskReddit/comments/5odcpd/which_movies_plot_would_drastically_change_if_you/">Which movie&rsquo;s plot would drastically change if you removed a letter from its title?</a>, submitted Monday, January 16th at 3:46 PM EST and receiving 4 upvotes (a &ldquo;good&rdquo; submission in context of this model). Plugging those input variables into the trained model results in a <strong>0.669</strong> probability of it being considered a good submission, which is consistent with the true results.</p>

<p>But what if we made <em>minor, iterative changes</em> to the title  while keeping the time submitted unchanged? Can we improve this probability?</p>

<p>&ldquo;Drastically&rdquo; is a silly adjective; removing it and using the title <strong>Which movie&rsquo;s plot would change if you removed a letter from its title?</strong> results in a greater probability of <strong>0.682</strong>.</p>

<p>&ldquo;Removed&rdquo; is <a href="http://www.ef.edu/english-resources/english-grammar/conditional/">grammatically incorrect</a>; fixing the issue and using the title <strong>Which movie&rsquo;s plot would change if you remove a letter from its title?</strong> results in a greater probability of <strong>0.692</strong>.</p>

<p>&ldquo;Which&rdquo; is also <a href="https://www.englishclub.com/vocabulary/wh-question-words.htm">grammatically incorrect</a>; fixing the issue and using the title <strong>What movie&rsquo;s plot would change if you remove a letter from its title?</strong> results in a greater probability of <strong>0.732</strong>.</p>

<p>Although adjectives are sometimes redundant, they can add an intriguing emphasis; adding a &ldquo;single&rdquo; and using the title <strong>What movie&rsquo;s plot would change if you remove a single letter from its title?</strong> results in a greater probability of <strong>0.753</strong>.</p>

<p>Not bad for a little workshopping!</p>

<p>Now that we have an improved title, we can find an optimal time to make the submission through brute force by calculating the probabilities for all combinations of hour, minute, and day of week (and offsetting the day of year appropriately). After doing so, I discovered that making the submission on the previous Sunday at 10:55 PM EST results in the maximum probability possible of being a good submission at <strong>0.841</strong> (the other top submission times are at various other minutes during that hour; the best time on a different day is the following Tuesday at 4:05 AM EST with a probability of <strong>0.823</strong>).</p>

<p>In all, this model of Reddit submission success prediction is a proof of concept; there are many, <em>many</em> optimizations that can be done on the feature engineering side and on the data collection side (especially if we want to model subreddits other than /r/AskReddit). Predicting which submissions go viral instead of just predicting which submissions receive atleast one upvote is another, more advanced problem entirely.</p>

<p>Thanks to the high-level abstractions and utility functions of Keras, I was able to prototype the initial model in an afternoon instead of the weeks/months required for academic papers and software applications in this area. At the least, this little experiment serves as an example of applying Keras to a real-world dataset, and the tradeoffs that result when deep learning can&rsquo;t magically solve everything. But that doesn&rsquo;t mean my experiments on the Reddit data were unproductive; on the contrary, I now have a few new clever ideas how to fix some of the issues discovered, which I hope to implement soon.</p>

<p>Again, I strongly recommend reading the data transformations and Keras code examples in <a href="https://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb">this Jupyter Notebook</a> for more information into the methodology, as building modern deep learning models is more intuitive and less arcane than what thought pieces on Medium imply.</p>

<hr>

<p><em>You can view the R and ggplot2 code used to visualize the model data in <a href="http://minimaxir.com/notebooks/predict-reddit-submission-success/">this R Notebook</a>, including 2D projections of the Embedding layers not in this article. You can also view the images/data used for this post in <a href="https://github.com/minimaxir/predict-reddit-submission-success">this GitHub repository</a>.</em></p>

<p><em>You are free to use the data visualizations/model architectures from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Decline of Imgur on Reddit and the Rise of Reddit's Native Image Hosting]]></title>
    <link href="http://minimaxir.com/2017/06/imgur-decline/"/>
    <updated>2017-06-20T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2017/06/imgur-decline</id>
    <content type="html"><![CDATA[<p>Last week, Bloomberg <a href="https://www.bloomberg.com/news/articles/2017-06-17/reddit-said-to-be-raising-funds-valuing-startup-at-1-7-billion">reported</a> that Reddit was raising about $150 Million in venture capital at a valuation of $1.7 billion. Since Reddit&rsquo;s data is <a href="http://minimaxir.com/2015/10/reddit-bigquery/">public on BigQuery</a>, I quickly checked if there were any recent user engagement growth spurts which could justify such a high worth. Here&rsquo;s an example BigQuery which aggregates the total number of Reddit submissions made for each month until the end of April 2017:</p>
<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="o">#</span><span class="n">standardSQL</span> 
<span class="k">SELECT</span> <span class="n">DATE_TRUNC</span><span class="p">(</span><span class="n">DATE</span><span class="p">(</span><span class="n">TIMESTAMP_SECONDS</span><span class="p">(</span><span class="n">created_utc</span><span class="p">)),</span> <span class="k">MONTH</span><span class="p">)</span> <span class="k">as</span> <span class="n">mon</span><span class="p">,</span>
  <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">as</span> <span class="n">num_submissions</span><span class="p">,</span>
  <span class="k">FROM</span> <span class="nv">`fh-bigquery.reddit_posts.*`</span>
  <span class="k">WHERE</span> <span class="p">(</span><span class="n">_TABLE_SUFFIX</span> <span class="k">BETWEEN</span> <span class="s1">'2016_01'</span> <span class="k">AND</span> <span class="s1">'2017_04'</span> <span class="k">OR</span> <span class="n">_TABLE_SUFFIX</span> <span class="o">=</span> <span class="s1">'full_corpus_201512'</span><span class="p">)</span>
  <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">mon</span>
  <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">mon</span>
</code></pre></div>
<p><img src="/img/imgur-decline/reddit-1.png" alt=""></p>

<p>As it turns out, Reddit did indeed get a large boost in activity toward the end of 2016, likely due to the <em>heated</em> discussions and events around the <a href="https://en.wikipedia.org/wiki/United_States_presidential_election,_2016">U.S. Presidential Election</a>. But Reddit has maintained the growth rate since then, which is very appealing to potential investors.</p>

<p>How are other sites benefiting from Reddit&rsquo;s growth? <a href="http://imgur.com">Imgur</a>, an image-host developed to be the <em>de facto</em> image hosting service for Reddit, shared in Reddit&rsquo;s continual growth&hellip;</p>

<p><img src="/img/imgur-decline/reddit-2.png" alt=""></p>

<p>&hellip;until mid-2016, when Imgur submission activity abruptly dropped. What happened?</p>

<p>Coincidentally in mid-2016, Reddit <a href="https://techcrunch.com/2016/05/25/reddit-image-uploads/">made itself</a> an image host for submissions to the site. Initially limited to uploads via the iOS/Android apps, Reddit then allowed desktop users to upload images through a <a href="https://www.reddit.com/r/changelog/comments/4kuk2j/reddit_change_introducing_image_uploading_beta/">beta rollout</a> starting May 24th, and a full <a href="https://www.reddit.com/r/announcements/comments/4p5dm9/image_hosting_on_reddit/">sitewide release</a> on June 21st.</p>

<p>How many Reddit-hosted image submissions are there compared to the number of Imgur submissions?</p>

<p><img src="/img/imgur-decline/reddit-3.png" alt=""></p>

<p>Wow, native Reddit images caught on.</p>

<h2>Market Share</h2>

<p><img src="/img/imgur-decline/pics.png" alt=""></p>

<p>Did the rise of Reddit-hosted images cause the decline of Imgur on Reddit? Let&rsquo;s look at the daily number of Imgur submissions and Reddit-hosted Image submissions from December 2015 to April 2017, normalized by the total number of sitewide submissions on that day. This gives us a Reddit &ldquo;market share&rdquo; metric for both services. </p>

<p>Additionally, we can plot vertical lines representing the dates when Reddit-hosted images rolled out in the limited beta release and the full sitewide release to see if there is a link between those events and submission behavior.</p>

<p><img src="/img/imgur-decline/reddit-4.png" alt=""></p>

<p>Before Reddit added native image hosting, Imgur accounted for 15% of all submissions to Reddit. Now it&rsquo;s below 9%. More Reddit-hosted images are being shared on Reddit than images from Imgur.</p>

<p>Instead of looking at all of Reddit, where spam subreddits could skew the results, we can also look at the largest image-only subreddits: <a href="https://www.reddit.com/r/pics/">/r/pics</a> and <a href="https://www.reddit.com/r/gifs/">/r/gifs</a>, both of which were a part of the beta rollout.</p>

<p><img src="/img/imgur-decline/reddit-5.png" alt=""></p>

<p>Here, the impact of the two rollouts is much noticeable, with immediate increases in Reddit-hosted image market share after each rollout, and proportional decreases in Imgur market share. The growth rate after the beta release is flat for both services, but when Reddit image hosting becomes sitewide, the market shares of Reddit-hosted/Imgur images increase/decrease linearly over time once users officially learn that the native image upload functionality exists. And these trends do not appear to be slowing down.</p>

<h2>A Silver Lining?</h2>

<p>Obviously Imgur does not like losing a <em>large</em> chunk of traffic, but there&rsquo;s a possibility that this outcome will be better for the business than what&rsquo;s implied from the charts above.</p>

<p>Hosting images on the internet isn&rsquo;t free, and bandwidth costs are the primary reason dedicated image hosts have died off over the years. Direct image links which show the user only the image and nothing else are convenient, but they are pure loss for the service. That&rsquo;s why image hosts encourage linking to the image on a landing page of the website, filled with ads which generate an expected revenue greater than the cost of serving the image.</p>

<p>After a user uploads an image to Imgur on the desktop, the user is given two share links that can be submitted to sites like Reddit: an image link that goes to the image + ads, and a direct link to the image.</p>

<p><img src="/img/imgur-decline/imgur_direct.png" alt=""></p>

<p>Recently, Imgur has <a href="https://www.reddit.com/r/assholedesign/comments/5gs96k/just_show_me_the_fucking_image_imgur/">pushed app downloads</a> when visiting the site on an iOS/Android device, including <a href="https://www.reddit.com/r/assholedesign/comments/695efj/upload_image_on_imgur_mobile_has_been_replaced_by/">disabling uploads</a> in the mobile browser. When sharing an image from the Imgur app, the <em>only</em> way to share an image is through the image link, which could lead to an increase in the proportion of ad-filled Imgur image links on Reddit. Said increase could counteract the decrease in total Imgur submissions, and Imgur could actually come out ahead.</p>

<p>With BigQuery, we can check the percentage of all Imgur submissions to Reddit which are direct links and the percentage which are indirect/lead to a landing page, and see if the ratio changes along the same time horizon used above:</p>

<p><img src="/img/imgur-decline/reddit-6.png" alt=""></p>

<p>Welp. No significant change in the ratio over time, eliminating that possible silver lining.</p>

<h2>Conclusion</h2>

<p>Note that the decline of Imgur on Reddit says nothing about Imgur as a business; it&rsquo;s entirely possible that Imgur&rsquo;s traffic on the main site itself is sufficient for growth. But the loss of Reddit traffic certainly can&rsquo;t be ignored, and it&rsquo;s interesting to visualize how quickly a service can be replaced when there&rsquo;s an equivalent native feature.</p>

<p>It&rsquo;s worth nothing that new competitors in the image space such as <a href="https://giphy.com">Giphy</a> utilize image hosting as a <em>secondary</em> service. Instead, they focus on building a repository of images which can be licensed and accessed programmatically by other services like Slack, Facebook, and Twitter. And Giphy has raised <a href="https://www.crunchbase.com/organization/giphy#/entity">$150 Million</a> total with this approach, so perhaps the image hosting market itself has indeed changed.</p>

<hr>

<p><em>You can view the R, ggplot2 code, and BigQueries used to visualize the Reddit data in <a href="http://minimaxir.com/notebooks/imgur-decline/">this R Notebook</a>. You can also view the images/data used for this post in <a href="https://github.com/minimaxir/imgur-decline">this GitHub repository</a></em>.</p>

<p><em>You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking CNTK on Keras: is it Better at Deep Learning than TensorFlow?]]></title>
    <link href="http://minimaxir.com/2017/06/keras-cntk/"/>
    <updated>2017-06-12T08:30:00-07:00</updated>
    <id>http://minimaxir.com/2017/06/keras-cntk</id>
    <content type="html"><![CDATA[<style>
div#htmlwidget_container {
  margin-bottom: 20px;
}
</style>

<p><a href="https://github.com/fchollet/keras">Keras</a> is a high-level open-source framework for deep learning, maintained by François Chollet, that abstracts the massive amounts of configuration and matrix algebra needed to build production-quality deep learning models. The Keras API abstracts a lower-level deep learning framework like <a href="https://github.com/Theano/Theano">Theano</a> or Google&rsquo;s <a href="https://www.tensorflow.org">TensorFlow</a> framework. Switching between these backends is only a matter of <a href="https://keras.io/backend/">setting flags</a>; no front-end code changes necessary.</p>

<p>But while Google has received a lot of publicity with TensorFlow, Microsoft has been quietly releasing their own machine learning frameworks open-source. There is <a href="https://github.com/Microsoft/LightGBM">LightGBM</a>, presented as an alternative to the extremely famous <a href="https://github.com/dmlc/xgboost">xgboost</a> framework. Now, there is <a href="https://github.com/Microsoft/CNTK">CNTK</a> (Microsoft Cognitive Toolkit), released at v2.0 a <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/ReleaseNotes/CNTK_2_0_Release_Notes">couple weeks ago</a>, which markets strong performance in both accuracy and speed even when <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/reasons-to-switch-from-tensorflow-to-cntk">compared</a> to TensorFlow.</p>

<p>CNTK v2.0 also has a key feature: Keras compatibility. And just last week, support for the CNTK backend <a href="https://github.com/fchollet/keras/pull/6800">was merged</a> into the official Keras repository.</p>

<p>Microsoft employees <a href="https://news.ycombinator.com/item?id=14470967">commented on Hacker News</a> that simply changing the backend of Keras from TensorFlow to CNTK would result in a performance boost. So let&rsquo;s put that to the test.</p>

<h2>Deep Learning in the Cloud</h2>

<p>Setting up a GPU-instance for deep learning in the cloud is surprisingly underdiscussed. Most recommend simply using a <a href="https://blog.keras.io/running-jupyter-notebooks-on-gpu-on-aws-a-starter-guide.html">premade image</a> from Amazon which includes all the necessary GPU drivers. However, Amazon EC2 <a href="https://aws.amazon.com/ec2/pricing/on-demand/">charges</a> $0.90/hr (not-prorated) for a NVIDIA Tesla K80 GPU instance, while Google Compute Engine <a href="https://cloud.google.com/compute/pricing#gpus">charges</a> $0.75/hr (prorated to the minute) for the same GPU, which is a nontrivial discount for the <em>many</em> hours necessary to train deep learning models.</p>

<p>The catch with GCE is you have to setup the deep learning drivers and frameworks from a blank Linux instance. I did that for my <a href="http://minimaxir.com/2017/04/char-embeddings/">first adventure</a> with Keras and it was not fun. However, I recently found <a href="https://medium.com/google-cloud/containerized-jupyter-notebooks-on-gpu-on-google-cloud-8e86ef7f31e9">a blog post</a> by Durgesh Mankekar which takes a more modern approach to managing such dependencies with <a href="https://www.docker.com">Docker</a> containers, and also provides a setup script plus <a href="https://github.com/durgeshm/dockerfiles/blob/master/jupyter-keras-gpu/Dockerfile">container</a> with the necessary deep learning drivers/frameworks for Keras. This container can then be loaded using <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a>, which allows Docker containers to access the GPU on the host. Running a deep learning script in the container is simply a matter of running a Docker command. After the script completes, the container is destroyed. This approach incidentally ensures that separate executions are independent; perfect for benchmarking/reproducibility.</p>

<p>I <a href="https://github.com/minimaxir/keras-cntk-docker">tweaked the container</a> to include an installation of CNTK, a CNTK-compatable version of Keras, and made CNTK the default backend for Keras.</p>

<p><img src="/img/keras-cntk/cntk_keras.gif" alt=""></p>

<h2>Benchmark Methodology</h2>

<p>The Keras <a href="https://github.com/fchollet/keras/tree/master/examples">examples</a> are robust and solve real-world deep learning problems; perfect for simulating real-world performance. I took <a href="https://github.com/minimaxir/keras-cntk-benchmark/tree/master/test_files">a variety</a> of those examples, emphasizing different neural network architectures, and added a <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/CustomCallback.py">custom logger</a> which outputs a CSV containing both model performance and elapsed time as the training progresses.</p>

<p>As mentioned earlier, the only change needed to switch between backends is setting a flag. Even though CNTK is the default backend for Keras in the container, a simple <code>-e KERAS_BACKEND=&#39;tensorflow&#39;</code> argument in the Docker command switches it to TensorFlow.</p>

<p><img src="/img/keras-cntk/tensorflow_keras_2.gif" alt=""></p>

<p>I wrote a Python <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/keras_cntk_benchmark.py">benchmark script</a> (executed on the host) to administrate and run all the examples in their own Docker containers, with both CNTK and TensorFlow, and collected the resulting logs.</p>

<p>Here are the results.</p>

<h2>IMDb Review Dataset</h2>

<p>The <a href="http://ai.stanford.edu/%7Eamaas/data/sentiment/">IMDb review dataset</a> is a famous dataset for benchmarking natural language processing (NLP) for sentiment analysis. The 25,000 reviews in the dataset are tagged as positive or negative. Good machine learning models developed <a href="http://ai.stanford.edu/%7Eamaas/papers/wvSent_acl2011.pdf">before deep learning became mainstream</a> score about 88% classification accuracy on the test dataset.</p>

<p>The first <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/imdb_bidirectional_lstm.py">model approach</a> is with a <strong>Bidirectional LSTM</strong>, which weights the model by the sequence of words, both forward <em>and</em> backward.</p>

<p>First, let&rsquo;s look at the classification accuracy of the test set at various points in time while the model is being trained:</p>

<p><em>Note: all charts in this blog post are interactive <a href="https://plot.ly">Plotly</a> charts; feel free to mouse-over data points for exact values and use the controls in the upper-right to manipulate the chart.</em></p>

<div id="htmlwidget_container">
  <div id="d5e3793d2a4e" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e3793d2a4e">{"x":{"data":[{"x":[1,2,3,4],"y":[0.85196,0.84492,0.83352,0.83544],"text":["epoch: 1<br />val_acc: 0.85196<br />framework: CNTK","epoch: 2<br />val_acc: 0.84492<br />framework: CNTK","epoch: 3<br />val_acc: 0.83352<br />framework: CNTK","epoch: 4<br />val_acc: 0.83544<br />framework: CNTK"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)","dash":"solid"},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4],"y":[0.84356,0.84584,0.836,0.83432],"text":["epoch: 1<br />val_acc: 0.84356<br />framework: TensorFlow","epoch: 2<br />val_acc: 0.84584<br />framework: TensorFlow","epoch: 3<br />val_acc: 0.83600<br />framework: TensorFlow","epoch: 4<br />val_acc: 0.83432<br />framework: TensorFlow"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)","dash":"solid"},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[4,3,2,1],"y":[0.83544,0.83352,0.84492,0.85196],"text":["epoch: 4<br />val_acc: 0.83544<br />framework: CNTK","epoch: 3<br />val_acc: 0.83352<br />framework: CNTK","epoch: 2<br />val_acc: 0.84492<br />framework: CNTK","epoch: 1<br />val_acc: 0.85196<br />framework: CNTK"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)"}},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[4,3,2,1],"y":[0.83432,0.836,0.84584,0.84356],"text":["epoch: 4<br />val_acc: 0.83432<br />framework: TensorFlow","epoch: 3<br />val_acc: 0.83600<br />framework: TensorFlow","epoch: 2<br />val_acc: 0.84584<br />framework: TensorFlow","epoch: 1<br />val_acc: 0.84356<br />framework: TensorFlow"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)"}},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":69.7384806973848},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Performance of Bidirectional LSTM Approach on IMDb Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.85,4.15],"tickmode":"array","ticktext":["1","2","3","4"],"tickvals":[1,2,3,4],"categoryorder":"array","categoryarray":["1","2","3","4"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Epoch","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.832598,0.852882],"tickmode":"array","ticktext":["83.5%","84.0%","84.5%","85.0%"],"tickvals":[0.835,0.84,0.845,0.85],"categoryorder":"array","categoryarray":["83.5%","84.0%","84.5%","85.0%"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Test Accuracy (Higher is Better)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e33b9e64b4":{"x":{},"y":{},"colour":{},"type":"scatter"},"d5e31be58400":{"x":{},"y":{},"colour":{}}},"cur_data":"d5e33b9e64b4","visdat":{"d5e33b9e64b4":["function (y) ","x"],"d5e31be58400":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e3793d2a4e">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>Normally the accuracy <em>increases</em> as training proceeds; Bidirectional LSTMs take a long time to train to get improving results, but at the least both frameworks are equally performant.</p>

<p>To gauge the speed of algorithm, we can calculate the average amount of time it takes to train an epoch (i.e. each time the model sees the entire training set). The time is mostly consistent per epoch but there is some variability; each measurement will have a 95% confidence interval for the true average, obtained via <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">nonparametric bootstrapping</a>. In the case of the Bidirectional LSTM:</p>

<div id="htmlwidget_container">
  <div id="d5e353d0aad9" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e353d0aad9">{"x":{"data":[{"orientation":"v","width":0.25,"base":0,"x":[1],"y":[152.378944575787],"text":"framework: CNTK<br />mean: 152.3789","type":"bar","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.25,"base":0,"x":[2],"y":[276.647240757942],"text":"framework: TensorFlow<br />mean: 276.6472","type":"bar","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[152.378944575787],"text":"framework: CNTK<br />mean: 152.3789","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[4.1145806312561],"arrayminus":[2.95618027448654],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[2],"y":[276.647240757942],"text":"framework: TensorFlow<br />mean: 276.6472","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[7.57349234819412],"arrayminus":[4.70573216676712],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":54.8609381486094},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Speed of Bidirectional LSTM Approach on IMDb Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.4,2.6],"tickmode":"array","ticktext":["CNTK","TensorFlow"],"tickvals":[1,2],"categoryorder":"array","categoryarray":["CNTK","TensorFlow"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Keras Backend","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[-14.2110366553068,298.431769761443],"tickmode":"array","ticktext":["0","100","200"],"tickvals":[0,100,200],"categoryorder":"array","categoryarray":["0","100","200"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Average Epoch Runtime (seconds)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e310f8be0a":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{},"type":"bar"},"d5e33a848b3a":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{}}},"cur_data":"d5e310f8be0a","visdat":{"d5e310f8be0a":["function (y) ","x"],"d5e33a848b3a":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e353d0aad9">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>Wow, CNTK is much faster! Not the 5x-10x speedup the <a href="https://arxiv.org/abs/1608.07249">benchmarks</a> highlighted for working with LSTMs, but nearly halving the runtime by simply setting a backend flag is still impressive.</p>

<p>Next, we&rsquo;ll look at the modern <strong>fasttext</strong> <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/imdb_fasttext.py">approach</a> on the same dataset. Fasttext is a newer algorithm that averages word vector Embeddings together (irrespective of order), but gets incredible results at incredible speeds even when using the CPU only, as with Facebook&rsquo;s <a href="https://github.com/facebookresearch/fastText">official implementation</a> for fasttext. (for this benchmark, I opt to include bigrams)</p>

<div id="htmlwidget_container">
  <div id="d5e3d468e4c" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e3d468e4c">{"x":{"data":[{"x":[1,2,3,4,5],"y":[0.85992,0.89428,0.90168,0.904,0.90636],"text":["epoch: 1<br />val_acc: 0.85992<br />framework: CNTK","epoch: 2<br />val_acc: 0.89428<br />framework: CNTK","epoch: 3<br />val_acc: 0.90168<br />framework: CNTK","epoch: 4<br />val_acc: 0.90400<br />framework: CNTK","epoch: 5<br />val_acc: 0.90636<br />framework: CNTK"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)","dash":"solid"},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5],"y":[0.86024,0.89416,0.90144,0.90384,0.90684],"text":["epoch: 1<br />val_acc: 0.86024<br />framework: TensorFlow","epoch: 2<br />val_acc: 0.89416<br />framework: TensorFlow","epoch: 3<br />val_acc: 0.90144<br />framework: TensorFlow","epoch: 4<br />val_acc: 0.90384<br />framework: TensorFlow","epoch: 5<br />val_acc: 0.90684<br />framework: TensorFlow"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)","dash":"solid"},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[5,4,3,2,1],"y":[0.90636,0.904,0.90168,0.89428,0.85992],"text":["epoch: 5<br />val_acc: 0.90636<br />framework: CNTK","epoch: 4<br />val_acc: 0.90400<br />framework: CNTK","epoch: 3<br />val_acc: 0.90168<br />framework: CNTK","epoch: 2<br />val_acc: 0.89428<br />framework: CNTK","epoch: 1<br />val_acc: 0.85992<br />framework: CNTK"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)"}},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[5,4,3,2,1],"y":[0.90684,0.90384,0.90144,0.89416,0.86024],"text":["epoch: 5<br />val_acc: 0.90684<br />framework: TensorFlow","epoch: 4<br />val_acc: 0.90384<br />framework: TensorFlow","epoch: 3<br />val_acc: 0.90144<br />framework: TensorFlow","epoch: 2<br />val_acc: 0.89416<br />framework: TensorFlow","epoch: 1<br />val_acc: 0.86024<br />framework: TensorFlow"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)"}},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":54.8609381486094},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Performance of fasttext Approach on IMDb Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.8,5.2],"tickmode":"array","ticktext":["1","2","3","4","5"],"tickvals":[1,2,3,4,5],"categoryorder":"array","categoryarray":["1","2","3","4","5"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Epoch","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.857574,0.909186],"tickmode":"array","ticktext":["86%","87%","88%","89%","90%"],"tickvals":[0.86,0.87,0.88,0.89,0.9],"categoryorder":"array","categoryarray":["86%","87%","88%","89%","90%"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Test Accuracy (Higher is Better)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e317b24b8b":{"x":{},"y":{},"colour":{},"type":"scatter"},"d5e33a7d9ed4":{"x":{},"y":{},"colour":{}}},"cur_data":"d5e317b24b8b","visdat":{"d5e317b24b8b":["function (y) ","x"],"d5e33a7d9ed4":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e3d468e4c">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<div id="htmlwidget_container">
  <div id="d5e34103705b" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e34103705b">{"x":{"data":[{"orientation":"v","width":0.5,"base":0,"x":[1],"y":[69.4701688766479],"text":"framework: CNTK<br />mean: 69.47017","type":"bar","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.5,"base":0,"x":[2],"y":[58.3061577320099],"text":"framework: TensorFlow<br />mean: 58.30616","type":"bar","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[69.4701688766479],"text":"framework: CNTK<br />mean: 69.47017","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.0613580703735437],"arrayminus":[0.0351073741912842],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[2],"y":[58.3061577320099],"text":"framework: TensorFlow<br />mean: 58.30616","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.416218662261961],"arrayminus":[0.21453107081912],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":47.4221668742217},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Speed of fasttext Approach on IMDb Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.4,2.6],"tickmode":"array","ticktext":["CNTK","TensorFlow"],"tickvals":[1,2],"categoryorder":"array","categoryarray":["CNTK","TensorFlow"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Keras Backend","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[-3.47657634735107,73.0081032943726],"tickmode":"array","ticktext":["0","20","40","60"],"tickvals":[0,20,40,60],"categoryorder":"array","categoryarray":["0","20","40","60"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Average Epoch Runtime (seconds)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e320cea520":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{},"type":"bar"},"d5e35ebae8b3":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{}}},"cur_data":"d5e320cea520","visdat":{"d5e320cea520":["function (y) ","x"],"d5e35ebae8b3":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e34103705b">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>Both frameworks have nearly identical accuracy due to model simplicity, but in this case, TensorFlow is faster at working with Embeddings. (at the least, fasttext clearly much faster than the Bidirectional LSTM approach!) In addition, fasttext blows away the 88% benchmark, which may be worth considering for other machine learning projects.</p>

<h2>MNIST Dataset</h2>

<p>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> is another famous dataset of handwritten digits, good for testing computer vision (60,000 training images, 10,000 test images). Generally, good models get above 99% classification accuracy on the test set.</p>

<p>The <strong>multilayer perceptron (MLP)</strong> <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/mnist_mlp.py">approach</a> just uses a large fully-connected network and lets <em>Deep Learning Magic &trade;</em> take over. Sometimes that can be enough.</p>

<div id="htmlwidget_container">
  <div id="d5e3134e61e6" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e3134e61e6">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],"y":[0.9584,0.9709,0.976,0.9802,0.9789,0.9831,0.984,0.9842,0.9835,0.9825,0.9823,0.9819,0.9842,0.9832,0.9839,0.9835,0.9838,0.9825,0.9833,0.9796],"text":["epoch:  1<br />val_acc: 0.9584<br />framework: CNTK","epoch:  2<br />val_acc: 0.9709<br />framework: CNTK","epoch:  3<br />val_acc: 0.9760<br />framework: CNTK","epoch:  4<br />val_acc: 0.9802<br />framework: CNTK","epoch:  5<br />val_acc: 0.9789<br />framework: CNTK","epoch:  6<br />val_acc: 0.9831<br />framework: CNTK","epoch:  7<br />val_acc: 0.9840<br />framework: CNTK","epoch:  8<br />val_acc: 0.9842<br />framework: CNTK","epoch:  9<br />val_acc: 0.9835<br />framework: CNTK","epoch: 10<br />val_acc: 0.9825<br />framework: CNTK","epoch: 11<br />val_acc: 0.9823<br />framework: CNTK","epoch: 12<br />val_acc: 0.9819<br />framework: CNTK","epoch: 13<br />val_acc: 0.9842<br />framework: CNTK","epoch: 14<br />val_acc: 0.9832<br />framework: CNTK","epoch: 15<br />val_acc: 0.9839<br />framework: CNTK","epoch: 16<br />val_acc: 0.9835<br />framework: CNTK","epoch: 17<br />val_acc: 0.9838<br />framework: CNTK","epoch: 18<br />val_acc: 0.9825<br />framework: CNTK","epoch: 19<br />val_acc: 0.9833<br />framework: CNTK","epoch: 20<br />val_acc: 0.9796<br />framework: CNTK"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)","dash":"solid"},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],"y":[0.9664,0.9773,0.9788,0.9764,0.981,0.9788,0.9792,0.9823,0.9823,0.984,0.9851,0.982,0.9838,0.9843,0.9855,0.9841,0.9818,0.9837,0.9832,0.9839],"text":["epoch:  1<br />val_acc: 0.9664<br />framework: TensorFlow","epoch:  2<br />val_acc: 0.9773<br />framework: TensorFlow","epoch:  3<br />val_acc: 0.9788<br />framework: TensorFlow","epoch:  4<br />val_acc: 0.9764<br />framework: TensorFlow","epoch:  5<br />val_acc: 0.9810<br />framework: TensorFlow","epoch:  6<br />val_acc: 0.9788<br />framework: TensorFlow","epoch:  7<br />val_acc: 0.9792<br />framework: TensorFlow","epoch:  8<br />val_acc: 0.9823<br />framework: TensorFlow","epoch:  9<br />val_acc: 0.9823<br />framework: TensorFlow","epoch: 10<br />val_acc: 0.9840<br />framework: TensorFlow","epoch: 11<br />val_acc: 0.9851<br />framework: TensorFlow","epoch: 12<br />val_acc: 0.9820<br />framework: TensorFlow","epoch: 13<br />val_acc: 0.9838<br />framework: TensorFlow","epoch: 14<br />val_acc: 0.9843<br />framework: TensorFlow","epoch: 15<br />val_acc: 0.9855<br />framework: TensorFlow","epoch: 16<br />val_acc: 0.9841<br />framework: TensorFlow","epoch: 17<br />val_acc: 0.9818<br />framework: TensorFlow","epoch: 18<br />val_acc: 0.9837<br />framework: TensorFlow","epoch: 19<br />val_acc: 0.9832<br />framework: TensorFlow","epoch: 20<br />val_acc: 0.9839<br />framework: TensorFlow"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)","dash":"solid"},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],"y":[0.9796,0.9833,0.9825,0.9838,0.9835,0.9839,0.9832,0.9842,0.9819,0.9823,0.9825,0.9835,0.9842,0.984,0.9831,0.9789,0.9802,0.976,0.9709,0.9584],"text":["epoch: 20<br />val_acc: 0.9796<br />framework: CNTK","epoch: 19<br />val_acc: 0.9833<br />framework: CNTK","epoch: 18<br />val_acc: 0.9825<br />framework: CNTK","epoch: 17<br />val_acc: 0.9838<br />framework: CNTK","epoch: 16<br />val_acc: 0.9835<br />framework: CNTK","epoch: 15<br />val_acc: 0.9839<br />framework: CNTK","epoch: 14<br />val_acc: 0.9832<br />framework: CNTK","epoch: 13<br />val_acc: 0.9842<br />framework: CNTK","epoch: 12<br />val_acc: 0.9819<br />framework: CNTK","epoch: 11<br />val_acc: 0.9823<br />framework: CNTK","epoch: 10<br />val_acc: 0.9825<br />framework: CNTK","epoch:  9<br />val_acc: 0.9835<br />framework: CNTK","epoch:  8<br />val_acc: 0.9842<br />framework: CNTK","epoch:  7<br />val_acc: 0.9840<br />framework: CNTK","epoch:  6<br />val_acc: 0.9831<br />framework: CNTK","epoch:  5<br />val_acc: 0.9789<br />framework: CNTK","epoch:  4<br />val_acc: 0.9802<br />framework: CNTK","epoch:  3<br />val_acc: 0.9760<br />framework: CNTK","epoch:  2<br />val_acc: 0.9709<br />framework: CNTK","epoch:  1<br />val_acc: 0.9584<br />framework: CNTK"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)"}},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,3,2,1,4],"y":[0.9839,0.9832,0.9837,0.9818,0.9841,0.9855,0.9843,0.9838,0.982,0.9851,0.984,0.9823,0.9823,0.9792,0.9788,0.981,0.9788,0.9773,0.9664,0.9764],"text":["epoch: 20<br />val_acc: 0.9839<br />framework: TensorFlow","epoch: 19<br />val_acc: 0.9832<br />framework: TensorFlow","epoch: 18<br />val_acc: 0.9837<br />framework: TensorFlow","epoch: 17<br />val_acc: 0.9818<br />framework: TensorFlow","epoch: 16<br />val_acc: 0.9841<br />framework: TensorFlow","epoch: 15<br />val_acc: 0.9855<br />framework: TensorFlow","epoch: 14<br />val_acc: 0.9843<br />framework: TensorFlow","epoch: 13<br />val_acc: 0.9838<br />framework: TensorFlow","epoch: 12<br />val_acc: 0.9820<br />framework: TensorFlow","epoch: 11<br />val_acc: 0.9851<br />framework: TensorFlow","epoch: 10<br />val_acc: 0.9840<br />framework: TensorFlow","epoch:  9<br />val_acc: 0.9823<br />framework: TensorFlow","epoch:  8<br />val_acc: 0.9823<br />framework: TensorFlow","epoch:  7<br />val_acc: 0.9792<br />framework: TensorFlow","epoch:  6<br />val_acc: 0.9788<br />framework: TensorFlow","epoch:  5<br />val_acc: 0.9810<br />framework: TensorFlow","epoch:  3<br />val_acc: 0.9788<br />framework: TensorFlow","epoch:  2<br />val_acc: 0.9773<br />framework: TensorFlow","epoch:  1<br />val_acc: 0.9664<br />framework: TensorFlow","epoch:  4<br />val_acc: 0.9764<br />framework: TensorFlow"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)"}},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":54.8609381486094},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Performance of MLP Approach on MNIST Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.0499999999999999,20.95],"tickmode":"array","ticktext":["5","10","15","20"],"tickvals":[5,10,15,20],"categoryorder":"array","categoryarray":["5","10","15","20"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Epoch","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.957045,0.986855],"tickmode":"array","ticktext":["96%","97%","98%"],"tickvals":[0.96,0.97,0.98],"categoryorder":"array","categoryarray":["96%","97%","98%"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Test Accuracy (Higher is Better)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e3506d9704":{"x":{},"y":{},"colour":{},"type":"scatter"},"d5e34ad5b0dc":{"x":{},"y":{},"colour":{}}},"cur_data":"d5e3506d9704","visdat":{"d5e3506d9704":["function (y) ","x"],"d5e34ad5b0dc":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e3134e61e6">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<div id="htmlwidget_container">
  <div id="d5e37c954f2c" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e37c954f2c">{"x":{"data":[{"orientation":"v","width":0.5,"base":0,"x":[1],"y":[2.76817538738251],"text":"framework: CNTK<br />mean: 2.768175","type":"bar","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.5,"base":0,"x":[2],"y":[3.37409842014313],"text":"framework: TensorFlow<br />mean: 3.374098","type":"bar","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[2.76817538738251],"text":"framework: CNTK<br />mean: 2.768175","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.0490834009408876],"arrayminus":[0.0264339576589818],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[2],"y":[3.37409842014313],"text":"framework: TensorFlow<br />mean: 3.374098","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.122406645406047],"arrayminus":[0.039851210974144],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":39.983395599834},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Speed of MLP Approach on MNIST Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.4,2.6],"tickmode":"array","ticktext":["CNTK","TensorFlow"],"tickvals":[1,2],"categoryorder":"array","categoryarray":["CNTK","TensorFlow"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Keras Backend","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[-0.174825253277459,3.67133031882663],"tickmode":"array","ticktext":["0","1","2","3"],"tickvals":[0,1,2,3],"categoryorder":"array","categoryarray":["0","1","2","3"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Average Epoch Runtime (seconds)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e3533c155a":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{},"type":"bar"},"d5e3d9df267":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{}}},"cur_data":"d5e3533c155a","visdat":{"d5e3533c155a":["function (y) ","x"],"d5e3d9df267":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e37c954f2c">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>Both frameworks train the model extremely quickly taking only a few seconds per epoch; there&rsquo;s no clear winner in terms of accuracy (although neither broke 99%), but CNTK is faster.</p>

<p>Another <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/mnist_cnn.py">approach</a> is the <strong>convolutional neural network (CNN)</strong>, which utilizes the inherent relationships between adjacent pixels and is a more logical architecture for image data.</p>

<div id="htmlwidget_container">
  <div id="d5e32ffdeaf8" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e32ffdeaf8">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12],"y":[0.9768,0.9827,0.9853,0.9865,0.9882,0.9883,0.9899,0.9899,0.9894,0.9895,0.9893,0.9892],"text":["epoch:  1<br />val_acc: 0.9768<br />framework: CNTK","epoch:  2<br />val_acc: 0.9827<br />framework: CNTK","epoch:  3<br />val_acc: 0.9853<br />framework: CNTK","epoch:  4<br />val_acc: 0.9865<br />framework: CNTK","epoch:  5<br />val_acc: 0.9882<br />framework: CNTK","epoch:  6<br />val_acc: 0.9883<br />framework: CNTK","epoch:  7<br />val_acc: 0.9899<br />framework: CNTK","epoch:  8<br />val_acc: 0.9899<br />framework: CNTK","epoch:  9<br />val_acc: 0.9894<br />framework: CNTK","epoch: 10<br />val_acc: 0.9895<br />framework: CNTK","epoch: 11<br />val_acc: 0.9893<br />framework: CNTK","epoch: 12<br />val_acc: 0.9892<br />framework: CNTK"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)","dash":"solid"},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12],"y":[0.9745,0.983,0.9866,0.9884,0.989,0.9891,0.9901,0.9899,0.9911,0.9911,0.9918,0.9916],"text":["epoch:  1<br />val_acc: 0.9745<br />framework: TensorFlow","epoch:  2<br />val_acc: 0.9830<br />framework: TensorFlow","epoch:  3<br />val_acc: 0.9866<br />framework: TensorFlow","epoch:  4<br />val_acc: 0.9884<br />framework: TensorFlow","epoch:  5<br />val_acc: 0.9890<br />framework: TensorFlow","epoch:  6<br />val_acc: 0.9891<br />framework: TensorFlow","epoch:  7<br />val_acc: 0.9901<br />framework: TensorFlow","epoch:  8<br />val_acc: 0.9899<br />framework: TensorFlow","epoch:  9<br />val_acc: 0.9911<br />framework: TensorFlow","epoch: 10<br />val_acc: 0.9911<br />framework: TensorFlow","epoch: 11<br />val_acc: 0.9918<br />framework: TensorFlow","epoch: 12<br />val_acc: 0.9916<br />framework: TensorFlow"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)","dash":"solid"},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[12,11,10,9,7,6,5,4,3,8,2,1],"y":[0.9892,0.9893,0.9895,0.9894,0.9899,0.9883,0.9882,0.9865,0.9853,0.9899,0.9827,0.9768],"text":["epoch: 12<br />val_acc: 0.9892<br />framework: CNTK","epoch: 11<br />val_acc: 0.9893<br />framework: CNTK","epoch: 10<br />val_acc: 0.9895<br />framework: CNTK","epoch:  9<br />val_acc: 0.9894<br />framework: CNTK","epoch:  7<br />val_acc: 0.9899<br />framework: CNTK","epoch:  6<br />val_acc: 0.9883<br />framework: CNTK","epoch:  5<br />val_acc: 0.9882<br />framework: CNTK","epoch:  4<br />val_acc: 0.9865<br />framework: CNTK","epoch:  3<br />val_acc: 0.9853<br />framework: CNTK","epoch:  8<br />val_acc: 0.9899<br />framework: CNTK","epoch:  2<br />val_acc: 0.9827<br />framework: CNTK","epoch:  1<br />val_acc: 0.9768<br />framework: CNTK"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)"}},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[12,11,10,9,8,7,6,5,4,3,2,1],"y":[0.9916,0.9918,0.9911,0.9911,0.9899,0.9901,0.9891,0.989,0.9884,0.9866,0.983,0.9745],"text":["epoch: 12<br />val_acc: 0.9916<br />framework: TensorFlow","epoch: 11<br />val_acc: 0.9918<br />framework: TensorFlow","epoch: 10<br />val_acc: 0.9911<br />framework: TensorFlow","epoch:  9<br />val_acc: 0.9911<br />framework: TensorFlow","epoch:  8<br />val_acc: 0.9899<br />framework: TensorFlow","epoch:  7<br />val_acc: 0.9901<br />framework: TensorFlow","epoch:  6<br />val_acc: 0.9891<br />framework: TensorFlow","epoch:  5<br />val_acc: 0.9890<br />framework: TensorFlow","epoch:  4<br />val_acc: 0.9884<br />framework: TensorFlow","epoch:  3<br />val_acc: 0.9866<br />framework: TensorFlow","epoch:  2<br />val_acc: 0.9830<br />framework: TensorFlow","epoch:  1<br />val_acc: 0.9745<br />framework: TensorFlow"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)"}},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":69.7384806973848},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Performance of CNN Approach on MNIST Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.45,12.55],"tickmode":"array","ticktext":["2.5","5.0","7.5","10.0","12.5"],"tickvals":[2.5,5,7.5,10,12.5],"categoryorder":"array","categoryarray":["2.5","5.0","7.5","10.0","12.5"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Epoch","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.973635,0.992665],"tickmode":"array","ticktext":["97.5%","98.0%","98.5%","99.0%"],"tickvals":[0.975,0.98,0.985,0.99],"categoryorder":"array","categoryarray":["97.5%","98.0%","98.5%","99.0%"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Test Accuracy (Higher is Better)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e31f8815b8":{"x":{},"y":{},"colour":{},"type":"scatter"},"d5e31f49f334":{"x":{},"y":{},"colour":{}}},"cur_data":"d5e31f8815b8","visdat":{"d5e31f8815b8":["function (y) ","x"],"d5e31f49f334":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e32ffdeaf8">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<div id="htmlwidget_container">
  <div id="d5e3638d0839" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e3638d0839">{"x":{"data":[{"orientation":"v","width":0.5,"base":0,"x":[1],"y":[15.8528816103935],"text":"framework: CNTK<br />mean: 15.85288","type":"bar","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.5,"base":0,"x":[2],"y":[11.0993242661158],"text":"framework: TensorFlow<br />mean: 11.09932","type":"bar","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[15.8528816103935],"text":"framework: CNTK<br />mean: 15.85288","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.431788841883343],"arrayminus":[0.114179053223424],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[2],"y":[11.0993242661158],"text":"framework: TensorFlow<br />mean: 11.09932","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.42946408474802],"arrayminus":[0.116898433586767],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":47.4221668742217},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Speed of CNN Approach on MNIST Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.4,2.6],"tickmode":"array","ticktext":["CNTK","TensorFlow"],"tickvals":[1,2],"categoryorder":"array","categoryarray":["CNTK","TensorFlow"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Keras Backend","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[-0.814233522613843,17.0989039748907],"tickmode":"array","ticktext":["0","5","10","15"],"tickvals":[0,5,10,15],"categoryorder":"array","categoryarray":["0","5","10","15"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Average Epoch Runtime (seconds)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e3421703f2":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{},"type":"bar"},"d5e3750426c3":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{}}},"cur_data":"d5e3421703f2","visdat":{"d5e3421703f2":["function (y) ","x"],"d5e3750426c3":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e3638d0839">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>In this case, TensorFlow performs better, both in accuracy <em>and</em> speed (and it breaks 99% too).</p>

<h2>CIFAR-10</h2>

<p>Going more into complex real-world models, the <a href="https://www.cs.toronto.edu/%7Ekriz/cifar.html">CIFAR-10 dataset</a> is a dataset used for image classification of 10 different objects. The architecture in the <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/cifar10_cnn.py">benchmark script</a> is a <strong>Deep CNN + MLP</strong> of many layers similar in architecture to the famous <a href="https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3">VGG-16</a> model, but more simple since most people do not have a super-computer cluster to train it.</p>

<div id="htmlwidget_container">
  <div id="d5e33466a90c" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e33466a90c">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],"y":[0.4301,0.4885,0.5267,0.5329,0.6017,0.6228,0.6412,0.6423,0.6695,0.6745,0.675,0.6939,0.7043,0.7066,0.6976,0.7036,0.7328,0.7259,0.7279,0.74],"text":["epoch:  1<br />val_acc: 0.4301<br />framework: CNTK","epoch:  2<br />val_acc: 0.4885<br />framework: CNTK","epoch:  3<br />val_acc: 0.5267<br />framework: CNTK","epoch:  4<br />val_acc: 0.5329<br />framework: CNTK","epoch:  5<br />val_acc: 0.6017<br />framework: CNTK","epoch:  6<br />val_acc: 0.6228<br />framework: CNTK","epoch:  7<br />val_acc: 0.6412<br />framework: CNTK","epoch:  8<br />val_acc: 0.6423<br />framework: CNTK","epoch:  9<br />val_acc: 0.6695<br />framework: CNTK","epoch: 10<br />val_acc: 0.6745<br />framework: CNTK","epoch: 11<br />val_acc: 0.6750<br />framework: CNTK","epoch: 12<br />val_acc: 0.6939<br />framework: CNTK","epoch: 13<br />val_acc: 0.7043<br />framework: CNTK","epoch: 14<br />val_acc: 0.7066<br />framework: CNTK","epoch: 15<br />val_acc: 0.6976<br />framework: CNTK","epoch: 16<br />val_acc: 0.7036<br />framework: CNTK","epoch: 17<br />val_acc: 0.7328<br />framework: CNTK","epoch: 18<br />val_acc: 0.7259<br />framework: CNTK","epoch: 19<br />val_acc: 0.7279<br />framework: CNTK","epoch: 20<br />val_acc: 0.7400<br />framework: CNTK"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)","dash":"solid"},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],"y":[0.4381,0.5066,0.5545,0.5603,0.6146,0.6296,0.6458,0.6396,0.6652,0.6786,0.6943,0.7035,0.7055,0.699,0.7159,0.7171,0.7327,0.7348,0.737,0.7453],"text":["epoch:  1<br />val_acc: 0.4381<br />framework: TensorFlow","epoch:  2<br />val_acc: 0.5066<br />framework: TensorFlow","epoch:  3<br />val_acc: 0.5545<br />framework: TensorFlow","epoch:  4<br />val_acc: 0.5603<br />framework: TensorFlow","epoch:  5<br />val_acc: 0.6146<br />framework: TensorFlow","epoch:  6<br />val_acc: 0.6296<br />framework: TensorFlow","epoch:  7<br />val_acc: 0.6458<br />framework: TensorFlow","epoch:  8<br />val_acc: 0.6396<br />framework: TensorFlow","epoch:  9<br />val_acc: 0.6652<br />framework: TensorFlow","epoch: 10<br />val_acc: 0.6786<br />framework: TensorFlow","epoch: 11<br />val_acc: 0.6943<br />framework: TensorFlow","epoch: 12<br />val_acc: 0.7035<br />framework: TensorFlow","epoch: 13<br />val_acc: 0.7055<br />framework: TensorFlow","epoch: 14<br />val_acc: 0.6990<br />framework: TensorFlow","epoch: 15<br />val_acc: 0.7159<br />framework: TensorFlow","epoch: 16<br />val_acc: 0.7171<br />framework: TensorFlow","epoch: 17<br />val_acc: 0.7327<br />framework: TensorFlow","epoch: 18<br />val_acc: 0.7348<br />framework: TensorFlow","epoch: 19<br />val_acc: 0.7370<br />framework: TensorFlow","epoch: 20<br />val_acc: 0.7453<br />framework: TensorFlow"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)","dash":"solid"},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],"y":[0.74,0.7279,0.7259,0.7328,0.7036,0.6976,0.7066,0.7043,0.6939,0.675,0.6745,0.6695,0.6423,0.6412,0.6228,0.6017,0.5329,0.5267,0.4885,0.4301],"text":["epoch: 20<br />val_acc: 0.7400<br />framework: CNTK","epoch: 19<br />val_acc: 0.7279<br />framework: CNTK","epoch: 18<br />val_acc: 0.7259<br />framework: CNTK","epoch: 17<br />val_acc: 0.7328<br />framework: CNTK","epoch: 16<br />val_acc: 0.7036<br />framework: CNTK","epoch: 15<br />val_acc: 0.6976<br />framework: CNTK","epoch: 14<br />val_acc: 0.7066<br />framework: CNTK","epoch: 13<br />val_acc: 0.7043<br />framework: CNTK","epoch: 12<br />val_acc: 0.6939<br />framework: CNTK","epoch: 11<br />val_acc: 0.6750<br />framework: CNTK","epoch: 10<br />val_acc: 0.6745<br />framework: CNTK","epoch:  9<br />val_acc: 0.6695<br />framework: CNTK","epoch:  8<br />val_acc: 0.6423<br />framework: CNTK","epoch:  7<br />val_acc: 0.6412<br />framework: CNTK","epoch:  6<br />val_acc: 0.6228<br />framework: CNTK","epoch:  5<br />val_acc: 0.6017<br />framework: CNTK","epoch:  4<br />val_acc: 0.5329<br />framework: CNTK","epoch:  3<br />val_acc: 0.5267<br />framework: CNTK","epoch:  2<br />val_acc: 0.4885<br />framework: CNTK","epoch:  1<br />val_acc: 0.4301<br />framework: CNTK"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)"}},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],"y":[0.7453,0.737,0.7348,0.7327,0.7171,0.7159,0.699,0.7055,0.7035,0.6943,0.6786,0.6652,0.6396,0.6458,0.6296,0.6146,0.5603,0.5545,0.5066,0.4381],"text":["epoch: 20<br />val_acc: 0.7453<br />framework: TensorFlow","epoch: 19<br />val_acc: 0.7370<br />framework: TensorFlow","epoch: 18<br />val_acc: 0.7348<br />framework: TensorFlow","epoch: 17<br />val_acc: 0.7327<br />framework: TensorFlow","epoch: 16<br />val_acc: 0.7171<br />framework: TensorFlow","epoch: 15<br />val_acc: 0.7159<br />framework: TensorFlow","epoch: 14<br />val_acc: 0.6990<br />framework: TensorFlow","epoch: 13<br />val_acc: 0.7055<br />framework: TensorFlow","epoch: 12<br />val_acc: 0.7035<br />framework: TensorFlow","epoch: 11<br />val_acc: 0.6943<br />framework: TensorFlow","epoch: 10<br />val_acc: 0.6786<br />framework: TensorFlow","epoch:  9<br />val_acc: 0.6652<br />framework: TensorFlow","epoch:  8<br />val_acc: 0.6396<br />framework: TensorFlow","epoch:  7<br />val_acc: 0.6458<br />framework: TensorFlow","epoch:  6<br />val_acc: 0.6296<br />framework: TensorFlow","epoch:  5<br />val_acc: 0.6146<br />framework: TensorFlow","epoch:  4<br />val_acc: 0.5603<br />framework: TensorFlow","epoch:  3<br />val_acc: 0.5545<br />framework: TensorFlow","epoch:  2<br />val_acc: 0.5066<br />framework: TensorFlow","epoch:  1<br />val_acc: 0.4381<br />framework: TensorFlow"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)"}},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":54.8609381486094},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Performance of CNN Approach on CIFAR-10 Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.0499999999999999,20.95],"tickmode":"array","ticktext":["5","10","15","20"],"tickvals":[5,10,15,20],"categoryorder":"array","categoryarray":["5","10","15","20"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Epoch","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.41434,0.76106],"tickmode":"array","ticktext":["50%","60%","70%"],"tickvals":[0.5,0.6,0.7],"categoryorder":"array","categoryarray":["50%","60%","70%"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Test Accuracy (Higher is Better)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e37bf8b19a":{"x":{},"y":{},"colour":{},"type":"scatter"},"d5e3454350c":{"x":{},"y":{},"colour":{}}},"cur_data":"d5e37bf8b19a","visdat":{"d5e37bf8b19a":["function (y) ","x"],"d5e3454350c":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e33466a90c">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<div id="htmlwidget_container">
  <div id="d5e33bb3e726" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e33bb3e726">{"x":{"data":[{"orientation":"v","width":0.5,"base":0,"x":[1],"y":[40.2903408885002],"text":"framework: CNTK<br />mean: 40.29034","type":"bar","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.5,"base":0,"x":[2],"y":[39.2076283454895],"text":"framework: TensorFlow<br />mean: 39.20763","type":"bar","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[40.2903408885002],"text":"framework: CNTK<br />mean: 40.29034","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.629282373999914],"arrayminus":[0.678987689315889],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[2],"y":[39.2076283454895],"text":"framework: TensorFlow<br />mean: 39.20763","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[1.48350351100346],"arrayminus":[0.962176468314958],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":47.4221668742217},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Speed of CNN Approach on CIFAR-10 Data","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.4,2.6],"tickmode":"array","ticktext":["CNTK","TensorFlow"],"tickvals":[1,2],"categoryorder":"array","categoryarray":["CNTK","TensorFlow"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Keras Backend","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[-2.04598116312501,42.9656044256251],"tickmode":"array","ticktext":["0","10","20","30","40"],"tickvals":[0,10,20,30,40],"categoryorder":"array","categoryarray":["0","10","20","30","40"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Average Epoch Runtime (seconds)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e362e819c7":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{},"type":"bar"},"d5e373f48a8b":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{}}},"cur_data":"d5e362e819c7","visdat":{"d5e362e819c7":["function (y) ","x"],"d5e373f48a8b":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e33bb3e726">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>In this case, performance between the two backends is <em>equal</em>, both in accuracy and speed. Perhaps the MLP benefits of CNTK and the CNN benefits of TensorFlow canceled each other out.</p>

<h2>Nietzsche Text Generation</h2>

<p><a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/lstm_text_generation.py">Text generation</a> based off of <a href="https://github.com/karpathy/char-rnn">char-rnn</a> is popular. Specifically, it uses a <strong>LSTM</strong> to &ldquo;learn&rdquo; the text and sample new text. In the Keras example using <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche">Nietzsche&rsquo;s</a> <a href="https://s3.amazonaws.com/text-datasets/nietzsche.txt">ramblings</a> as the source dataset, the model attempts to predict the next character using the previous 40 characters, and minimize the training loss. Ideally you want below 1.00 loss before generated text is grammatically coherent.</p>

<div id="htmlwidget_container">
  <div id="d5e39946fc" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e39946fc">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10],"y":[2.00440777019,1.64802151896,1.55469932735,1.50813395339,1.47896788624,1.45615055283,1.44040952227,1.42692558066,1.41612182536,1.40472281286],"text":["epoch:  1<br />loss: 2.004408<br />framework: CNTK","epoch:  2<br />loss: 1.648022<br />framework: CNTK","epoch:  3<br />loss: 1.554699<br />framework: CNTK","epoch:  4<br />loss: 1.508134<br />framework: CNTK","epoch:  5<br />loss: 1.478968<br />framework: CNTK","epoch:  6<br />loss: 1.456151<br />framework: CNTK","epoch:  7<br />loss: 1.440410<br />framework: CNTK","epoch:  8<br />loss: 1.426926<br />framework: CNTK","epoch:  9<br />loss: 1.416122<br />framework: CNTK","epoch: 10<br />loss: 1.404723<br />framework: CNTK"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)","dash":"solid"},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10],"y":[2.01015738712,1.66685410016,1.55497078145,1.50765904726,1.47890785292,1.45558360526,1.44119170534,1.42519511551,1.41509616279,1.40604660672],"text":["epoch:  1<br />loss: 2.010157<br />framework: TensorFlow","epoch:  2<br />loss: 1.666854<br />framework: TensorFlow","epoch:  3<br />loss: 1.554971<br />framework: TensorFlow","epoch:  4<br />loss: 1.507659<br />framework: TensorFlow","epoch:  5<br />loss: 1.478908<br />framework: TensorFlow","epoch:  6<br />loss: 1.455584<br />framework: TensorFlow","epoch:  7<br />loss: 1.441192<br />framework: TensorFlow","epoch:  8<br />loss: 1.425195<br />framework: TensorFlow","epoch:  9<br />loss: 1.415096<br />framework: TensorFlow","epoch: 10<br />loss: 1.406047<br />framework: TensorFlow"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)","dash":"solid"},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,9,8,7,6,5,4,2,3,1],"y":[1.40472281286,1.41612182536,1.42692558066,1.44040952227,1.45615055283,1.47896788624,1.50813395339,1.64802151896,1.55469932735,2.00440777019],"text":["epoch: 10<br />loss: 1.404723<br />framework: CNTK","epoch:  9<br />loss: 1.416122<br />framework: CNTK","epoch:  8<br />loss: 1.426926<br />framework: CNTK","epoch:  7<br />loss: 1.440410<br />framework: CNTK","epoch:  6<br />loss: 1.456151<br />framework: CNTK","epoch:  5<br />loss: 1.478968<br />framework: CNTK","epoch:  4<br />loss: 1.508134<br />framework: CNTK","epoch:  2<br />loss: 1.648022<br />framework: CNTK","epoch:  3<br />loss: 1.554699<br />framework: CNTK","epoch:  1<br />loss: 2.004408<br />framework: CNTK"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)"}},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,9,8,7,6,5,4,3,2,1],"y":[1.40604660672,1.41509616279,1.42519511551,1.44119170534,1.45558360526,1.47890785292,1.50765904726,1.55497078145,1.66685410016,2.01015738712],"text":["epoch: 10<br />loss: 1.406047<br />framework: TensorFlow","epoch:  9<br />loss: 1.415096<br />framework: TensorFlow","epoch:  8<br />loss: 1.425195<br />framework: TensorFlow","epoch:  7<br />loss: 1.441192<br />framework: TensorFlow","epoch:  6<br />loss: 1.455584<br />framework: TensorFlow","epoch:  5<br />loss: 1.478908<br />framework: TensorFlow","epoch:  4<br />loss: 1.507659<br />framework: TensorFlow","epoch:  3<br />loss: 1.554971<br />framework: TensorFlow","epoch:  2<br />loss: 1.666854<br />framework: TensorFlow","epoch:  1<br />loss: 2.010157<br />framework: TensorFlow"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)"}},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":54.8609381486094},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Performance of Text Generation via LSTM","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.55,10.45],"tickmode":"array","ticktext":["2.5","5.0","7.5","10.0"],"tickvals":[2.5,5,7.5,10],"categoryorder":"array","categoryarray":["2.5","5.0","7.5","10.0"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Epoch","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[1.374451084147,2.040429115833],"tickmode":"array","ticktext":["1.4","1.6","1.8","2.0"],"tickvals":[1.4,1.6,1.8,2],"categoryorder":"array","categoryarray":["1.4","1.6","1.8","2.0"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Loss (Lower is Better)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e32c23029d":{"x":{},"y":{},"colour":{},"type":"scatter"},"d5e32e80a80e":{"x":{},"y":{},"colour":{}}},"cur_data":"d5e32c23029d","visdat":{"d5e32c23029d":["function (y) ","x"],"d5e32e80a80e":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e39946fc">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<div id="htmlwidget_container">
  <div id="d5e31d02de77" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e31d02de77">{"x":{"data":[{"orientation":"v","width":0.5,"base":0,"x":[1],"y":[46.3590185880661],"text":"framework: CNTK<br />mean: 46.35902","type":"bar","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.5,"base":0,"x":[2],"y":[87.6180793523788],"text":"framework: TensorFlow<br />mean: 87.61808","type":"bar","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[46.3590185880661],"text":"framework: CNTK<br />mean: 46.35902","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[1.3510494601399],"arrayminus":[0.473184503636254],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[2],"y":[87.6180793523788],"text":"framework: TensorFlow<br />mean: 87.61808","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[1.02050027749789],"arrayminus":[0.707233857780466],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":47.4221668742217},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Speed of Text Generation via LSTM","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.4,2.6],"tickmode":"array","ticktext":["CNTK","TensorFlow"],"tickvals":[1,2],"categoryorder":"array","categoryarray":["CNTK","TensorFlow"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Keras Backend","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[-4.43192898149384,93.0705086113706],"tickmode":"array","ticktext":["0","25","50","75"],"tickvals":[0,25,50,75],"categoryorder":"array","categoryarray":["0","25","50","75"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Average Epoch Runtime (seconds)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e32f5a5bdc":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{},"type":"bar"},"d5e35544e0cd":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{}}},"cur_data":"d5e32f5a5bdc","visdat":{"d5e32f5a5bdc":["function (y) ","x"],"d5e35544e0cd":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e31d02de77">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>Both have similar changes in loss over time (unfortunately, a loss of 1.40 will still result in gibberish text generated), although performance on CTNK is again fast due to the LSTM architecture.</p>

<p>For this next benchmark, I will <em>not</em> use a official Keras example script, but instead use my <em>own</em> <a href="https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/text_generator_keras.py">text generator architecture</a>, created during <a href="http://minimaxir.com/2017/04/char-embeddings/">my previous Keras post</a>.</p>

<div id="htmlwidget_container">
  <div id="d5e33e1d4f93" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e33e1d4f93">{"x":{"data":[{"x":[1,2,3,4,5,6,7,8,9,10],"y":[2.00351653112,1.72485264239,1.60276587148,1.51993696254,1.46022373265,1.41141036682,1.37179022428,1.33587299931,1.3062139421,1.27900991821],"text":["epoch:  1<br />loss: 2.003517<br />framework: CNTK","epoch:  2<br />loss: 1.724853<br />framework: CNTK","epoch:  3<br />loss: 1.602766<br />framework: CNTK","epoch:  4<br />loss: 1.519937<br />framework: CNTK","epoch:  5<br />loss: 1.460224<br />framework: CNTK","epoch:  6<br />loss: 1.411410<br />framework: CNTK","epoch:  7<br />loss: 1.371790<br />framework: CNTK","epoch:  8<br />loss: 1.335873<br />framework: CNTK","epoch:  9<br />loss: 1.306214<br />framework: CNTK","epoch: 10<br />loss: 1.279010<br />framework: CNTK"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)","dash":"solid"},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1,2,3,4,5,6,7,8,9,10],"y":[2.00852505003,1.73037641162,1.60431502849,1.52106773144,1.45870362669,1.40930885724,1.36821400507,1.33404665547,1.30228422424,1.27453109394],"text":["epoch:  1<br />loss: 2.008525<br />framework: TensorFlow","epoch:  2<br />loss: 1.730376<br />framework: TensorFlow","epoch:  3<br />loss: 1.604315<br />framework: TensorFlow","epoch:  4<br />loss: 1.521068<br />framework: TensorFlow","epoch:  5<br />loss: 1.458704<br />framework: TensorFlow","epoch:  6<br />loss: 1.409309<br />framework: TensorFlow","epoch:  7<br />loss: 1.368214<br />framework: TensorFlow","epoch:  8<br />loss: 1.334047<br />framework: TensorFlow","epoch:  9<br />loss: 1.302284<br />framework: TensorFlow","epoch: 10<br />loss: 1.274531<br />framework: TensorFlow"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)","dash":"solid"},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,9,8,7,6,5,4,3,2,1],"y":[1.27900991821,1.3062139421,1.33587299931,1.37179022428,1.41141036682,1.46022373265,1.51993696254,1.60276587148,1.72485264239,2.00351653112],"text":["epoch: 10<br />loss: 1.279010<br />framework: CNTK","epoch:  9<br />loss: 1.306214<br />framework: CNTK","epoch:  8<br />loss: 1.335873<br />framework: CNTK","epoch:  7<br />loss: 1.371790<br />framework: CNTK","epoch:  6<br />loss: 1.411410<br />framework: CNTK","epoch:  5<br />loss: 1.460224<br />framework: CNTK","epoch:  4<br />loss: 1.519937<br />framework: CNTK","epoch:  3<br />loss: 1.602766<br />framework: CNTK","epoch:  2<br />loss: 1.724853<br />framework: CNTK","epoch:  1<br />loss: 2.003517<br />framework: CNTK"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(41,128,185,1)"}},"hoveron":"points","name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[10,9,8,7,6,5,4,3,2,1],"y":[1.27453109394,1.30228422424,1.33404665547,1.36821400507,1.40930885724,1.45870362669,1.52106773144,1.60431502849,1.73037641162,2.00852505003],"text":["epoch: 10<br />loss: 1.274531<br />framework: TensorFlow","epoch:  9<br />loss: 1.302284<br />framework: TensorFlow","epoch:  8<br />loss: 1.334047<br />framework: TensorFlow","epoch:  7<br />loss: 1.368214<br />framework: TensorFlow","epoch:  6<br />loss: 1.409309<br />framework: TensorFlow","epoch:  5<br />loss: 1.458704<br />framework: TensorFlow","epoch:  4<br />loss: 1.521068<br />framework: TensorFlow","epoch:  3<br />loss: 1.604315<br />framework: TensorFlow","epoch:  2<br />loss: 1.730376<br />framework: TensorFlow","epoch:  1<br />loss: 2.008525<br />framework: TensorFlow"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","opacity":1,"size":7.55905511811024,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(192,57,43,1)"}},"hoveron":"points","name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":54.8609381486094},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Performance of Text Generation via Custom Keras Model","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.55,10.45],"tickmode":"array","ticktext":["2.5","5.0","7.5","10.0"],"tickvals":[2.5,5,7.5,10],"categoryorder":"array","categoryarray":["2.5","5.0","7.5","10.0"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Epoch","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[1.2378313961355,2.0452247478345],"tickmode":"array","ticktext":["1.4","1.6","1.8","2.0"],"tickvals":[1.4,1.6,1.8,2],"categoryorder":"array","categoryarray":["1.4","1.6","1.8","2.0"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Loss (Lower is Better)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e3711b058f":{"x":{},"y":{},"colour":{},"type":"scatter"},"d5e3250a294c":{"x":{},"y":{},"colour":{}}},"cur_data":"d5e3711b058f","visdat":{"d5e3711b058f":["function (y) ","x"],"d5e3250a294c":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e33e1d4f93">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<div id="htmlwidget_container">
  <div id="d5e357be0802" style="width:100%;height:400px;" class="plotly html-widget"></div>
</div>

<script type="application/json" data-for="d5e357be0802">{"x":{"data":[{"orientation":"v","width":0.5,"base":0,"x":[1],"y":[100.614623141289],"text":"framework: CNTK<br />mean: 100.6146","type":"bar","marker":{"autocolorscale":false,"color":"rgba(41,128,185,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"CNTK","legendgroup":"CNTK","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"orientation":"v","width":0.5,"base":0,"x":[2],"y":[108.959185552597],"text":"framework: TensorFlow<br />mean: 108.9592","type":"bar","marker":{"autocolorscale":false,"color":"rgba(192,57,43,1)","line":{"width":1.88976377952756,"color":"transparent"}},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[100.614623141289],"text":"framework: CNTK<br />mean: 100.6146","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[1.08758223846512],"arrayminus":[0.724098751095781],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"CNTK","legendgroup":"CNTK","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[2],"y":[108.959185552597],"text":"framework: TensorFlow<br />mean: 108.9592","type":"scatter","mode":"lines","opacity":1,"line":{"color":"transparent"},"error_y":{"array":[0.933447687045287],"arrayminus":[0.662856574837988],"type":"data","width":19.8863636363636,"symmetric":false,"color":"rgba(0,0,0,1)"},"name":"TensorFlow","legendgroup":"TensorFlow","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":53.3612287256123,"r":9.29846409298464,"b":53.1686176836862,"l":47.4221668742217},"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"title":"Speed of Text Generation via Custom Keras Model","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":22.3163138231631},"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[0.4,2.6],"tickmode":"array","ticktext":["CNTK","TensorFlow"],"tickvals":[1,2],"categoryorder":"array","categoryarray":["CNTK","TensorFlow"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":"Keras Backend","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"range":[-5.49463166198212,115.387264901624],"tickmode":"array","ticktext":["0","30","60","90"],"tickvals":[0,30,60,90],"categoryorder":"array","categoryarray":["0","30","60","90"],"nticks":null,"ticks":"","tickcolor":null,"ticklen":4.64923204649232,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"Source Sans Pro","size":14.8775425487754},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":"Average Epoch Runtime (seconds)","titlefont":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":18.5969281859693},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"Source Sans Pro","size":14.8775425487754},"y":0.877694488188976},"hovermode":"closest","width":null,"height":400,"barmode":"relative"},"config":{"doubleClick":"reset","modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"source":"A","attrs":{"d5e3314208e1":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{},"type":"bar"},"d5e36655050a":{"x":{},"y":{},"ymin":{},"ymax":{},"fill":{}}},"cur_data":"d5e3314208e1","visdat":{"d5e3314208e1":["function (y) ","x"],"d5e36655050a":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>

<script type="application/htmlwidget-sizing" data-for="d5e357be0802">{"viewer":{"width":"100%","height":400,"padding":15,"fill":false},"browser":{"width":"100%","height":400,"padding":0,"fill":false}}</script>

<p>My network avoids converging early with only a minor cost to training speed in the TensorFlow case; unfortunately, CNTK speed is <em>much</em> slower than the simple model, but still faster than TensorFlow in the advanced model.</p>

<p>Here&rsquo;s the generated text output from the TensorFlow-trained model on my architecture:</p>
<div class="highlight"><pre><code class="language-" data-lang="">hinks the rich man must be wholly perverity and connection of the english sin of the philosophers of the basis of the same profound of his placed and evil and exception of fear to plants to me such as the case of the will seems to the will to be every such a remark as a primates of a strong of
[...]
</code></pre></div>
<p>And here&rsquo;s the output from the CNTK-trained model:</p>
<div class="highlight"><pre><code class="language-" data-lang="">(_x2js1hevjg4z_?z_aæ?q_gpmj:sn![?(f3_ch=lhw4y n6)gkh
kujau
momu,?!ljë7g)k,!?[45 0as9[d.68éhhptvsx jd_næi,ä_z!cwkr"_f6ë-mu_(epp
[...]
</code></pre></div>
<p>Wait, what? Apparently my model architecture caused CNTK to hit a legitimate bug when making predictions, which did not happen with CNTK + the simple LSTM architecture. Thanks to my QA skills, I found that <a href="https://keras.io/layers/normalization/">batch normalization</a> was the cause of the bug and <a href="https://github.com/Microsoft/CNTK/issues/1994">filed the issue</a> appropriately.</p>

<h2>Conclusion</h2>

<p>In all, the title of this post does not follow <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines">Betteridge&rsquo;s law of headlines</a>; deciding the better Keras framework is not as clear cut as expected. Accuracy is mostly identical between the two frameworks. CNTK is faster at LSTMs/MLPs, TensorFlow is faster at CNNs/Embeddings, but when networks implement <em>both</em>, it&rsquo;s a tie.</p>

<p>Random bug aside, it&rsquo;s possible that CNTK is not fully optimized for running on Keras (indeed, the 1bit-SGD functionality <a href="https://github.com/Microsoft/CNTK/issues/1975">does not work</a> yet) so there is still room for future improvement. Despite that, the results for simply setting a flag are <em>extremely</em> impressive, and it is worth testing Keras models on both CNTK and TensorFlow now to see which is better before deploying them to production.</p>

<hr>

<p><em>All scripts for running the benchmark are available in <a href="https://github.com/minimaxir/keras-cntk-benchmark">this GitHub repo</a>. You can view the R/ggplot2 code used to process the logs and create the interactive visualizations in <a href="http://minimaxir.com/notebooks/keras-cntk/">this R Notebook</a>.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advantages of Using R Notebooks For Data Analysis Instead of Jupyter Notebooks]]></title>
    <link href="http://minimaxir.com/2017/06/r-notebooks/"/>
    <updated>2017-06-06T08:30:00-07:00</updated>
    <id>http://minimaxir.com/2017/06/r-notebooks</id>
    <content type="html"><![CDATA[<p><a href="http://jupyter.org">Jupyter Notebooks</a>, formerly known as <a href="https://ipython.org/notebook.html">IPython Notebooks</a>, are ubiquitous in modern data analysis. The Notebook format allows statistical code and its output to be viewed on any computer in a logical and <em>reproducible</em> manner, avoiding both the confusion caused by unclear code and the inevitable &ldquo;it only works on my system&rdquo; curse.</p>

<p><img src="/img/r-notebooks/jupyterdemo.png" alt=""></p>

<p>In Jupyter Notebooks, each block of Python input code executes in its own cell, and the output of the block appears inline; this allows the user to iterate on the results, both to make the data transformations explicit and to and make sure the results are as expected.</p>

<p><img src="/img/r-notebooks/jupyter.png" alt=""></p>

<p>In addition to code blocks, Jupyter Notebooks support <a href="https://en.wikipedia.org/wiki/Markdown">Markdown</a> cells, allowing for more detailed write-ups with easy formatting. The final Notebook can be exported as a HTML file displayable in a browser, or the raw Notebook file can be shared and <a href="https://github.com/blog/1995-github-jupyter-notebooks-3">rendered</a> on sites like <a href="https://github.com">GitHub</a>. Although Jupyter is a Python application, it can run kernels of <a href="https://irkernel.github.io">non-Python languages</a>, such as <a href="https://www.r-project.org">R</a>.</p>

<p>Over the years, there have a been a few new competitors in the reproducible data analysis field, such as <a href="http://beakernotebook.com/features">Beaker Notebook</a> and, for heavy-duty business problems, <a href="https://zeppelin.apache.org">Apache Zeppelin</a>. However, today we&rsquo;ll look at the relatively new <a href="http://rmarkdown.rstudio.com/r_notebooks.html">R Notebooks</a>, and how they help improve the workflows of common data analysis in ways Jupyter Notebooks can&rsquo;t without third-party extensions.</p>

<h2>About R Notebooks</h2>

<p>R Notebooks are a format maintained by <a href="https://www.rstudio.com">RStudio</a>, which develops and maintains a large number of open source R packages and tools, most notably the free-for-consumer RStudio R IDE. More specifically, R Notebooks are an extension of the earlier <a href="http://rmarkdown.rstudio.com">R Markdown</a> <code>.Rmd</code> format, useful for rendering analyses into HTML/PDFs, or other cool formats like <a href="http://rmarkdown.rstudio.com/tufte_handout_format.html">Tufte handouts</a> or even <a href="https://bookdown.org">books</a>. The default output of an R Notebook file is a <code>.nb.html</code> file, which can be viewed as a webpage on any system. (<a href="https://rpubs.com">RPubs</a> has many examples of R Notebooks, although I recommend using <a href="https://pages.github.com">GitHub Pages</a> to host notebooks publicly).</p>

<p><img src="/img/r-notebooks/RNotebookAnimation.gif" alt=""></p>

<p>Instead of having separate cells for code and text, a R Markdown file is all plain text. The cells are indicated by three backticks and a gray background in RStudio, which makes it easy to enter a code block, easy to identify code blocks at a glance, and easy to execute a notebook block-by-block. Each cell also has a green indicator bar which shows which code is running and which code is queued, line-by-line.</p>

<p>For Notebook files, a HTML webpage is automatically generated whenever the file is saved, which can immediately be viewed in any browser (the generated webpage stores the cell output and any necessary dependencies).</p>

<p><img src="/img/r-notebooks/notebooktest.png" alt=""></p>

<p>R Notebooks can only be created and edited in RStudio, but this is a case where tight vertical integration of open-source software is a good thing. Among many other features, RStudio includes a file manager, a function help, a variable explorer, and a project manager; all of which make analysis much easier and faster as opposed to the browser-only Jupyter.</p>

<p><img src="/img/r-notebooks/rstudio.png" alt=""></p>

<p>I&rsquo;ve made many, many Jupyter Notebooks and R Notebooks <a href="http://minimaxir.com/data-portfolio">over the years</a>, which has given me insight into the strengths and weaknesses of both formats. Here are a few native features of R Notebooks which present an objective advantage over Jupyter Notebooks, particularly those not highlighted in the documentation:</p>

<h2>Version Control</h2>

<p>Version control of files with tools such as <a href="https://en.wikipedia.org/wiki/Git">git</a> is important as it both maintains an explorable database of changes to the code files and also improves collaboration by using a centralized server (e.g. GitHub) where anyone with access to the repository can pull and push changes to the code. In the data science world, large startups such as <a href="https://stripe.com/blog/reproducible-research">Stripe</a> and <a href="https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091">Airbnb</a> have seen a lot of success with this approach.</p>

<p>RStudio incidentally has a native git client for tracking and committing changes to a <code>.Rmd</code> file, which is easy since <code>.Rmd</code> files are effectively plain text files where you can see differences between versions at a per-line level. (You may not want to store the changes to the generated <code>.nb.html</code> Notebook since they will be large and redundant to the changes made in the corresponding <code>.Rmd</code>; I recommend adding a <code>*.nb.html</code> rule to a <code>.gitignore</code> file during analysis).</p>

<p><img src="/img/r-notebooks/git.png" alt=""></p>

<p>The <code>.ipynb</code> Jupyter Notebook files are blobs of JSON that also store cell output, which will result in large diffs if you keep them in version control and make any changes which result in different output. This can cause the git database to balloon and makes reading per-line diffs hard if not impossible.</p>

<p>On Hacker News, the version control issues in Jupyter are <a href="https://news.ycombinator.com/item?id=14034341">a common complaint</a>, however a Jupyter developer noted of a possibility of <a href="https://news.ycombinator.com/item?id=14035158">working with RStudio</a> on solving this issue.</p>

<h2>Inline Code Rendering</h2>

<p>A common practice in Jupyter Notebooks is to print common values as a part of a write-up or testing statistical code. In Jupyter Notebooks, if you want to verify the number of rows in a dataset for exploratory data analysis, you have to add an appropriate print statement to the cell to get the number <code>n</code> rows, and then add a Markdown cell to redundantly describe what you just print in the output. </p>

<p>In R Notebooks, you can skip a step by calling such print statements in-line in the Markdown text, which will then be rendered with the Notebook. This also avoids hard-coding such numbers in the Markdown text if you change the data beforehand (e.g. parameter tuning) or if the values are nontrivial to calculate by hand.</p>

<p>For example, these lines of R Markdown from my <a href="http://minimaxir.com/notebooks/first-comment/">Reddit First Comment Notebook</a>:</p>

<p><img src="/img/r-notebooks/inline.png" alt=""></p>

<p>translate into:</p>

<p><img src="/img/r-notebooks/reddit.png" alt=""></p>

<h2>Metadata</h2>

<p>R Notebooks are configured with a <a href="http://yaml.org">YAML</a> header, which can include common attributes such as title, author, date published, and other relevant options. These fields will then be configured correctly in the metadata for HTML/PDF/Handouts output. Here&rsquo;s an example from <a href="http://minimaxir.com/notebooks/amazon-spark/">one of my notebooks</a>:</p>
<div class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="nn">---</span>
<span class="na">title</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Playing</span><span class="nv"> </span><span class="s">with</span><span class="nv"> </span><span class="s">80</span><span class="nv"> </span><span class="s">Million</span><span class="nv"> </span><span class="s">Amazon</span><span class="nv"> </span><span class="s">Product</span><span class="nv"> </span><span class="s">Review</span><span class="nv"> </span><span class="s">Ratings</span><span class="nv"> </span><span class="s">Using</span><span class="nv"> </span><span class="s">Apache</span><span class="nv"> </span><span class="s">Spark"</span>
<span class="na">author</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Max</span><span class="nv"> </span><span class="s">Woolf</span><span class="nv"> </span><span class="s">(@minimaxir)"</span>
<span class="na">date</span><span class="pi">:</span> <span class="s2">"</span><span class="s">January</span><span class="nv"> </span><span class="s">2nd,</span><span class="nv"> </span><span class="s">2017"</span>
<span class="na">output</span><span class="pi">:</span>
  <span class="na">html_notebook</span><span class="pi">:</span>
    <span class="na">highlight</span><span class="pi">:</span> <span class="s">tango</span>
    <span class="na">mathjax</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">number_sections</span><span class="pi">:</span> <span class="s">yes</span>
    <span class="na">theme</span><span class="pi">:</span> <span class="s">spacelab</span>
    <span class="na">toc</span><span class="pi">:</span> <span class="s">yes</span>
    <span class="na">toc_float</span><span class="pi">:</span> <span class="s">yes</span>
<span class="nn">---</span>
</code></pre></div>
<p>Said metadata features are <a href="https://github.com/ipython/ipython/issues/6073">often requested but unimplemented</a> in Jupyter.</p>

<h2>Notebook Theming</h2>

<p>As noted in the example metadata above, R Notebooks allow extensive theming. Jupyter Notebooks do <a href="https://github.com/dunovank/jupyter-themes">support themes</a>, but with a third-party Python package, or placing custom CSS in an <a href="https://stackoverflow.com/a/32158550">odd location</a>.</p>

<p>Like Jupyter Notebooks, the front-end of browser-based R Notebooks is based off of the <a href="http://getbootstrap.com">Bootstrap</a> HTML framework. R Notebooks, however, allow you to natively select the style of code syntax highlighting via <code>highlight</code> (similar options as <a href="https://help.farbox.com/pygments.html">pygments</a>) and also the entire Bootstrap theme via <code>theme</code> (with a selection from the excellent <a href="https://bootswatch.com">Bootswatch</a> themes by <a href="https://twitter.com/thomashpark">Thomas Park</a>), giving your Notebook a unique look without adding dependencies.</p>

<h2>Data Tables</h2>

<p>When you print a data frame in a Jupyter Notebook, the output appears as a standard <em>boring</em> HTML table:</p>

<p><img src="/img/r-notebooks/htmltable.png" alt=""></p>

<p>No cell block output is ever truncated. Accidentally printing an entire 100,000+ row table to a Jupyter Notebook is a mistake you only make <em>once</em>.</p>

<p>R Notebook tables are pretty tables with pagination for both rows and columns, and can support large amounts of data if necessary.</p>

<p><img src="/img/r-notebooks/rtable.png" alt=""></p>

<p>The R Notebook output table also includes the data type of the column, which is helpful for debugging unexpected issues where a column has an unintended data type (e.g. a numeric <code>&lt;dbl&gt;</code> column or a datetime <code>&lt;S3: POSIXct&gt;</code> column is parsed as a text-based <code>&lt;chr&gt;</code> column).</p>

<h2>Table of Contents</h2>

<p>A Table of Contents always helps navigating, particularly in a PDF export. Jupyter Notebooks <a href="https://github.com/minrk/ipython_extensions">requires an extension</a> for a ToC, while R Notebooks will natively create one from section headers (controllable via <code>toc</code> and <code>number_sections</code>). An optional <code>toc_float</code> parameter causes the Table of Contents to float on the left in the browser, making it always accessible.</p>

<p><img src="/img/r-notebooks/notebookheader.png" alt=""></p>

<p>In conclusion, R Notebooks haven&rsquo;t received much publicity since the benefits aren&rsquo;t immediately obvious, but for the purpose of reproducible analyses, the breadth of native features allows for excellent utility while avoiding dependency hell. Running R in an R Notebook is a significantly better experience than running R in a Jupyter Notebook. The advantages present in R Notebooks can also provide guidance for feature development in other Notebook software, which improves the data analysis ecosystem as a whole.</p>

<p>However, there&rsquo;s an elephant in the room&hellip;</p>

<h2>What About Python?</h2>

<p>So you might be thinking &ldquo;an R Notebook forces you to use R, but <em>serious</em> data science work is done using Python!&rdquo; Plot twist: you can use Python in an R Notebook!</p>

<p><img src="/img/r-notebooks/python.png" alt=""></p>

<p>Well, sort of. The Python session ends after the cell executes, making it unhelpful for tasks other than <em>ad hoc</em> scripts.</p>

<p>The topic on whether R or Python is better for data analysis is a <a href="https://news.ycombinator.com/item?id=14056098">common</a> <a href="https://news.ycombinator.com/item?id=13239530">religious</a> <a href="https://news.ycombinator.com/item?id=12301996">flamewar</a> topic which is best saved for a separate blog post (tl;dr: I disagree with the paraphrased quote above in that both languages have their advantages and you&rsquo;ll benefit significantly from knowing both ecosystems).</p>

<p>And I wouldn&rsquo;t count R out of &ldquo;serious data science&rdquo;. You can use R <a href="http://spark.rstudio.com">seamlessly</a> with big data tools like <a href="https://spark.apache.org">Apache Spark</a>, and R can <a href="https://rstudio.github.io/keras/">now</a> use <a href="https://keras.io">Keras</a>/<a href="https://www.tensorflow.org">TensorFlow</a> for deep learning with near-API-parity to the Python version. <em>Hmm</em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Leaving Apple Inc.]]></title>
    <link href="http://minimaxir.com/2017/05/leaving-apple/"/>
    <updated>2017-05-04T09:30:00-07:00</updated>
    <id>http://minimaxir.com/2017/05/leaving-apple</id>
    <content type="html"><![CDATA[<p>I’ve been working in the San Francisco Bay Area for about 5 years, but I’ve never publicly said where I’ve worked. Well, I was a Software QA Engineer at <a href="https://www.apple.com">Apple Inc.</a>, on the Applications team.</p>

<p>As of last week, I handed in my resignation. While I am thankful for the opportunities I have had at Apple, it is time for me to pursue working in other areas I am passionate about and search for other companies to further my personal growth and technical skills. Resigning from a good job to look for something new might defy conventional wisdom, but the time is right for me to make this bold career move.</p>

<h2>My Apple Story</h2>

<p>I graduated with university honors at <a href="http://www.cmu.edu">Carnegie Mellon University</a>, from the <a href="http://tepper.cmu.edu">Tepper School of Business</a> with a focus on Computing and Information Technology (i.e. data architecture and coding algorithms), and a minor in Statistics.</p>

<p>At the end of my senior year, I received an e-mail from a Software QA Manager at Apple (who followed my <a href="http://techcommntr.tumblr.com">comments</a> at the bottom of <a href="https://techcrunch.com">TechCrunch</a> articles) inviting me for an on-site interview. Following an offer, I moved to the Bay Area to start my first post-undergrad job in Cupertino.</p>

<p>While I can&rsquo;t really talk about what I worked on at Apple, I genuinely enjoyed the work, the product, and team. I had a high impact on the final result and I successfully helped qualify many major software releases. However, after a few years, I realized that my technical skill growth was stalling, so I looked for an an internal transfer to another department, ideally in a data analysis/software engineering role.</p>

<p>Having received no responses internally, I realized I would have to expand my search to outside of Apple.</p>

<h2>My Job Hunt</h2>

<p>I have a strong technical background from my CMU classes, but not having an explicit Computer Science degree has made it difficult to prove aptitude despite my positive annual reviews and proven experience / technical skills at Apple. So I made the decision to blog with a technical focus here at <a href="http://minimaxir.com">minimaxir.com</a>, which gave me an avenue to showcase my programmatic skills and the opportunity to self-learn practical new tools not covered during the school curriculum, such as <a href="https://www.python.org">Python</a>, <a href="http://ggplot2.org">ggplot2</a>, version control with <a href="https://git-scm.com">git</a>, and reproducible analyses via <a href="http://jupyter.org">Jupyter/IPython Notebooks</a>.</p>

<p>This approach has been successful and many readers have liked my my blog posts: often topping <a href="https://www.reddit.com/r/dataisbeautiful/comments/4bwr7o/relationship_between_rotten_tomatoes_tomatometer/">Reddit</a> and <a href="https://news.ycombinator.com/item?id=13429656">Hacker News</a>, driving hundreds of thousands of pageviews. Additionally, a couple of my posts were even cited in larger publications such as the <a href="https://www.washingtonpost.com/news/the-intersect/wp/2016/06/30/facebook-news-feed-and-the-tyranny-of-positive-content/">Washington Post</a> and <a href="https://www.buzzfeed.com/tomphillips/photos-that-prove-game-of-thrones-happened-in-real-life">BuzzFeed</a>.</p>

<p>I also published many open-source technical projects to my <a href="https://github.com/minimaxir">GitHub</a>. My <a href="https://github.com/minimaxir/big-list-of-naughty-strings">Big List of Naughty Strings</a>, a project I made in a couple hours on a weekend inspired by my QA-ing at work, is now at <strong>20,000+ Stars</strong> on GitHub. My <a href="https://github.com/minimaxir/facebook-page-post-scraper">Facebook Page Post Scraper</a>, which does what the name implies, is now at 1,000+ Stars and has been used by many other businesses and journalists.</p>

<p>Developers have long argued that job seekers should have a strong public portfolio, as demonstrated experience can account for the lack of a relevant degree. After years of building up my portfolio, it became apparent that most outside recruiters I talked with never looked at my blog/GitHub, despite a strong emphasis of both on my résumé.</p>

<p>I subsequently rededicated my blog as a pragmatic demonstration of relevant skills in the data analysis job market, focusing more on practical analysis instead of quirky insights and thoughts. In the process, I obtained proficiency in a number of modern tools, including <a href="http://minimaxir.com/2016/08/clickbait-cluster/">interactive data visualizations</a> on the web with <a href="https://plot.ly">Plotly</a>, processing <a href="http://minimaxir.com/2017/01/amazon-spark/">big data</a> with <a href="http://spark.apache.org">Apache Spark</a>, high-performance <a href="http://minimaxir.com/2017/02/predicting-arrests/">machine learning</a> with <a href="https://github.com/dmlc/xgboost">xgboost</a> and <a href="https://github.com/Microsoft/LightGBM">LightGBM</a>, and even <a href="http://minimaxir.com/2017/04/char-embeddings/">deep learning</a> with <a href="https://github.com/fchollet/keras">Keras</a> and <a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>.</p>

<p>I am now actively looking for a <strong>data analyst/software engineering job within San Francisco</strong>. If you are interested or if you know of companies who are looking for qualified people, please send me an email at <strong><a href="mailto:max@minimaxir.com">max@minimaxir.com</a></strong>.</p>

<h2>Next Steps</h2>

<p>So I’ll be using my time over the next couple weeks to openly look for a new job, and to network with others in relevant industries (and be able to interview without taking a day off of work). Things have been improving: my <a href="https://news.ycombinator.com/item?id=14238066">comment</a> in the Hacker News &ldquo;Who wants to be hired?&rdquo; thread generated many leads who really liked my blog/portfolio. If you’d like to meet up in San Francisco and talk about tech and data stuff, just let me know.</p>

<p>I still intend to continue blogging, not as a hobby but in a more purposeful way. I have very ambitious goals and now have more time to execute them at a deeper level. Plans include:</p>

<ul>
<li>Web applications leveraging deep learning models, deployed at scale with <a href="https://www.docker.com">Docker</a>/<a href="https://kubernetes.io">Kubernetes</a>.</li>
<li>Interactive data dashboards accompanying every analytical blog post with <a href="https://shiny.rstudio.com">Shiny</a>.</li>
<li>Code screencasts at 4k resolution on <a href="https://youtube.com/minimaxir">YouTube</a>.</li>
<li>Data analysis live-streaming with augmented functionality on <a href="https://www.twitch.tv/minimaxir">Twitch</a>.</li>
</ul>

<p>I have set up a <strong><a href="https://www.patreon.com/minimaxir">Patreon</a></strong> in order to subsidize my machine learning/deep learning/software/hardware needs for my blog posts. If you have found any of my blog posts useful, a monetary contribution to my Patreon would be appreciated and will be put to good creative use.</p>

<p>If you want to keep up with me and my projects, feel free to follow me on <strong><a href="https://www.facebook.com/max.woolf">Facebook</a></strong> and <strong><a href="https://twitter.com/minimaxir">Twitter</a></strong> too.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pretrained Character Embeddings for Deep Learning and Automatic Text Generation]]></title>
    <link href="http://minimaxir.com/2017/04/char-embeddings/"/>
    <updated>2017-04-04T06:30:00-07:00</updated>
    <id>http://minimaxir.com/2017/04/char-embeddings</id>
    <content type="html"><![CDATA[<p>Deep learning is the biggest, <a href="http://approximatelycorrect.com/2017/03/28/the-ai-misinformation-epidemic/">often misapplied</a> buzzword nowadays for getting pageviews on blogs. As a result, there have been a lot of shenanigans lately with deep learning thought pieces and how deep learning can solve <em>anything</em> and make childhood sci-fi dreams come true.</p>

<p>I&rsquo;m not a fan of <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/ClarkesThirdLaw">Clarke&rsquo;s Third Law</a>, so I spent some time checking out deep learning myself. As it turns out, with modern deep learning tools like <a href="https://github.com/fchollet/keras">Keras</a>, a higher-level framework on top of the popular <a href="https://www.tensorflow.org">TensorFlow</a> framework, deep learning is <strong>easy to learn and understand</strong>. Yes, easy. And it <em>definitely</em> does not require a PhD, or even a Computer Science undergraduate degree, to implement models or make decisions based on the output.</p>

<p>However, let&rsquo;s try something more expansive than the stereotypical deep learning tutorials.</p>

<h2>Characters Welcome</h2>

<p>Word embeddings have been a popular machine learning trick nowadays. By using an algorithm such as <a href="https://en.wikipedia.org/wiki/Word2vec">Word2vec</a>, you can obtain a numeric representation of a word, and use those values to create numeric representations of higher-level representations like sentences/paragraphs/documents/etc.</p>

<p><img src="/img/char-embeddings/word-vectors.png" alt=""></p>

<p>However, generating word vectors for datasets can be computationally expensive (see <a href="http://minimaxir.com/2016/08/clickbait-cluster/">my earlier post</a> which uses Apache Spark/Word2vec to create sentence vectors at scale quickly). The academic way to work around this is to use pretrained word embeddings, such as <a href="https://nlp.stanford.edu/projects/glove/">the GloVe vectors</a> collected by researchers at Stanford NLP. However, GloVe vectors are huge; the largest one (840 billion tokens at 300D) is 5.65 GB on disk and may hit issues when loaded into memory on less-powerful computers.</p>

<p>Why not work <em>backwards</em> and calculate <em>character</em> embeddings? Then you could calculate a relatively few amount of vectors which would easily fit into memory, and use those to derive word vectors, which can then be used to derive the sentence/paragraph/document/etc vectors. But training character embeddings traditionally is significantly more computationally expensive since there are 5-6x the amount of tokens, and I don&rsquo;t have access to the supercomputing power of Stanford researchers.</p>

<p>Why not use the <em>existing</em> pretrained word embeddings to extrapolate the corresponding character embeddings within the word? Think &ldquo;<a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words</a>,&rdquo; except &ldquo;bag-of-characters.&rdquo; For example, from the embeddings from the word &ldquo;the&rdquo;, we can infer the embeddings for &ldquo;t&rdquo;, &ldquo;h,&rdquo; and &ldquo;e&rdquo; from the parent word, and average the t/h/e vectors from <em>all</em> words/tokens in the dataset corpus. (For this post, I will only look at the 840B/300D dataset since that is the only one with capital letters, which are rather important. If you want to use a dataset with smaller dimensionality, apply <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> on the final results)</p>

<p>I wrote a <a href="https://github.com/minimaxir/char-embeddings/blob/master/create_embeddings.py">simple Python script</a> that takes in the specified pretrained word embeddings and does just that, <a href="https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt">outputting the character embeddings</a> in the same format. (for simplicity, only ASCII characters are included; the <a href="https://en.wikipedia.org/wiki/Extended_ASCII">extended ASCII characters</a> are  intentionally omitted due to compatibility reasons. Additionally, by construction, space and newline characters are not represented in the derived dataset.)</p>

<p><img src="/img/char-embeddings/char-embeddings.png" alt=""></p>

<p>You may be thinking that I&rsquo;m cheating. So let&rsquo;s set a point-of-reference. Colin Morris <a href="http://colinmorris.github.io/blog/1b-words-char-embeddings">found</a> that when 16D character embeddings from a model used in Google&rsquo;s <a href="https://arxiv.org/abs/1312.3005">One Billion Word Benchmark</a> are projected into a 2D space via t-SNE, patterns emerge: digits are close, lowercase and uppercase letters are often paired, and punctuation marks are loosely paired.</p>

<p><img src="/img/char-embeddings/tsne_embeddings.png" alt=""></p>

<p>Let&rsquo;s do that for my derived character embeddings, but with <a href="https://www.r-project.org">R</a> and <a href="http://docs.ggplot2.org/current/">ggplot2</a>. t-SNE is <a href="http://distill.pub/2016/misread-tsne/">difficult to use</a> for high-dimensional vectors as combinations of parameters can result in wildly different output, so let&rsquo;s try a couple projections. Here&rsquo;s what happens when my pretrained projections are preprojected from 300D to 16D via <a href="http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/">PCA whitening</a>, and setting perplexity (number of optimal neighbors) to 7.</p>

<p><img src="/img/char-embeddings/char-tsne.png" alt=""></p>

<p>The algorithm manages to separate and group lowercase, uppercase, and numerals rather distinctly. Quadrupling the dimensionality of the preprocessing step to 64D and changing perplexity to 2 generates a depiction closer to the Google model projection:</p>

<p><img src="/img/char-embeddings/char-tsne-2.png" alt=""></p>

<p>My pretrained character embeddings trick isn&rsquo;t academic, but it&rsquo;s successfully identifying realistic relationships. There might be something here worthwhile.</p>

<h2>The Coolness of Deep Learning</h2>

<p>Keras, maintained by Google employee <a href="https://twitter.com/fchollet">François Chollet</a>, is so good that it is effectively cheating in the field of machine learning, where even TensorFlow tutorials can be replaced with a single line of code. (which is important for iteration; Keras layers are effectively Lego blocks). A simple read of the <a href="https://github.com/fchollet/keras/tree/master/examples">Keras examples</a> and <a href="https://keras.io/">documentation</a> will let you reverse-engineer most the revolutionary deep learning clickbait thought pieces. Some create entire startups by changing the source dataset of the Keras examples and pitch them to investors none-the-wiser, or make very light wrappers on top the examples for teaching tutorial videos and get thousands of subscribers on YouTube.</p>

<p>I prefer to parse documentation/examples as a proof-of-concept, but never as gospel. Examples are often not the most efficient ways to implement a solution to a problem, just merely a start. In the case of Keras&rsquo;s <a href="https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py">text generator example</a>, the initial code was likely modeled after the 2015 blog post <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> by Andrej Karpathy and the corresponding project <a href="https://github.com/karpathy/char-rnn">char-rnn</a>. There have been many new developments in neural network architecture since 2015 that can improve both speed and performance of the text generation model as a whole.</p>

<h2>What Text to Generate?</h2>

<p>The Keras example uses <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche">Nietzsche</a> writings as a data source, which I&rsquo;m not fond of because it&rsquo;s difficult to differentiate bad autogenerated Nietzsche rants from actual Nietzsche rants. What I want to generate is text with <em>rules</em>, with the algorithm being judged by how well it follows an inherent structure. My idea is to create <a href="http://magic.wizards.com/en">Magic: The Gathering</a> cards.</p>

<p><img src="/img/char-embeddings/dragon-whelp.jpg" alt=""></p>

<p>Inspired by the <a href="https://twitter.com/RoboRosewater">@RoboRosewater</a> Twitter account by Reed Milewicz and the <a href="http://www.mtgsalvation.com/forums/creativity/custom-card-creation/612057-generating-magic-cards-using-deep-recurrent-neural">corresponding research</a> and <a href="https://motherboard.vice.com/en_us/article/the-ai-that-learned-magic-the-gathering">articles</a>, I aim to see if it&rsquo;s possible to recreate the structured design creativity for myself.</p>

<p>Even if you are not familiar with Magic and its rules, you can still find the <a href="https://twitter.com/RoboRosewater/status/756198572282949632">card text</a> of RoboRosewater cards hilarious:</p>

<p><img src="/img/char-embeddings/horse.jpeg" alt=""></p>

<p>Occasionally RoboRosewater, using a weaker model, produces amusing <a href="https://twitter.com/RoboRosewater/status/689184317721960448">neural network trainwrecks</a>:</p>

<p><img src="/img/char-embeddings/carl.png" alt=""></p>

<p>More importantly, all Magic cards have an explicit structure; they have a name, mana cost in the upper-right, card type, card text, and usually a power and toughness in the bottom-right.</p>

<p>I wrote <a href="https://github.com/minimaxir/char-embeddings/blob/master/create_magic_text.py">another Python script</a> to parse all Magic card data from <a href="https://mtgjson.com">MTG JSON</a> into an encoding which matches this architecture, where each section transition has its own symbol delimiter, along with other encoding simplicities. For example, here is the card <a href="http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=247314">Dragon Whelp</a> in my encoding:</p>
<div class="highlight"><pre><code class="language-" data-lang="">[Dragon Whelp@{2}{R}{R}#Creature — Dragon$Flying|{R}: ~ gets +1/+0 until end of turn. If this ability has been activated four or more times this turn, sacrifice ~ at the beginning of the next end step.%2^3]
</code></pre></div>
<p>These card encodings are all combined into one .txt file, which will be fed into the model.</p>

<h2>Building and Training the Model</h2>

<p>The Keras text generation example operates by breaking a given .txt file into 40-character sequences, and the model tries to predict the 41st character by outputting a probability for each possible character (108 in this dataset). For example, if the input based on the above example is <code>[&#39;D&#39;, &#39;r&#39;, &#39;a&#39;, &#39;g&#39;, ..., &#39;D&#39;, &#39;r&#39;, &#39;a&#39;, &#39;g&#39;]</code> (with the latter Drag being part of the creature type), the model will optimize for outputting a probability of 1.0 of <code>o</code>; per the <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">categorical crossentropy</a> loss function, the model is rewarded for assigning correct guesses with 1.0 probability and incorrect guesses with 0.0 probabilities, penalizing half-guesses and wrong guesses.</p>

<p>Each possible 40-character sequence is collected, however only every other third sequence is kept; this prevents the model from being able to learn card text verbatim, plus it also makes training faster. (for this model, there are about <strong>1 million</strong> sequences for the final training). The example uses only a 128-node <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long-short-term-memory</a> (LSTM) <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a> (RNN) layer, popular for incorporating a &ldquo;memory&rdquo; into a neural network model, but the example notes at the beginning it can take awhile to train before generated text is coherent.</p>

<p>There are a few optimizations we can make. Instead of supplying the characters directly to the RNN, we can first encode them using an <a href="https://keras.io/layers/embeddings/">Embedding layer</a> so the model can train character context. We can stack more layers on the RNN by adding a 2-level <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a>: a <a href="https://www.reddit.com/r/ProgrammerHumor/comments/5si1f0/machine_learning_approaches/">meme</a>, yes, but it helps, as the network must learn latent representations of the data. Thanks to recent developments such as <a href="https://arxiv.org/abs/1502.03167">batch normalization</a> and <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear activations</a> for these <a href="https://keras.io/layers/core/#dense">Dense layers</a>, they can both be trained without as much computational overhead, and thanks to Keras, both can be added to a layer with a single line of code each. Lastly, we can add an auxiliary output via Keras&rsquo;s <a href="https://keras.io/models/model/">functional API</a> where the network makes a prediction based on only the output from the RNN in addition to the main output, which forces it to work smarter and ends up resulting in a <em>significant</em> improvement in loss for the main path.</p>

<p>The final architecture ends up looking like this:</p>

<p><img src="/img/char-embeddings/model.png" alt=""></p>

<p>And because we added an Embedding layer, we can load the pretrained 300D character embeds I made earlier, giving the model a good start in understanding character relationships.</p>

<p>The goal of the training is to minimize the total loss of the model. (but for evaluating model performance, we only look at the loss of the main output). The model is trained in <strong>epochs</strong>, where the model sees all the input data atleast once. During each epoch, batches of size 128 are loaded into the model and evaluated, calculating a <strong>batch loss</strong> for each; the gradients from the batch are backpropagated into the previous layers to improve them. While training with Keras, the console reports an <strong>epoch loss</strong>, which is the average of all the batch losses so far in the current epoch, allowing the user to see in real time how the model improves, and it&rsquo;s addicting.</p>

<p><img src="/img/char-embeddings/keras-training.gif" alt=""></p>

<p>Keras/TensorFlow works just fine on the CPU, but for models with a RNN, you&rsquo;ll want to consider using a GPU for performance, specifically one by nVidia. Amazon has cloud GPU instances for $0.90/hr (<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html">not prorated</a>), but very recently, Google announced <a href="https://cloud.google.com/compute/docs/gpus/add-gpus">GPU instances</a> of the same caliber for ~$0.75/hr (prorated to the minute), which is what I used to train this model, although Google Compute Engine requires configuring the GPU drivers first. For 20 epochs, it took about 4 hours and 20 minutes to train the model while spending $3.26, which isn&rsquo;t bad as far as deep learning goes.</p>

<h2>Making Magic</h2>

<p>After each epoch, the original Keras text generation example takes a sentence from the input data as a seed and predicts the next character in the sequence according to the model, then uses the last 40 characters generated for the next character, etc. The sampling incorporates a diversity/temperature parameter which allows the model to make suboptimal decisions and select characters with lower natural probabilities, which allows for the romantic &ldquo;creativity&rdquo; popular with neural network text generation.</p>

<p>With the Magic card dataset and my tweaked model architecture, generated text is coherent <a href="https://github.com/minimaxir/char-embeddings/blob/master/output/iter-01-0_9204.txt">after the 1st epoch</a>! After about 20 epochs, training becomes super slow, but the predicted text becomes super interesting. Here are a few fun examples from a <a href="https://github.com/minimaxir/char-embeddings/blob/master/output/text_sample.txt">list of hundreds of generated cards</a>. (Note: the power/toughness values at the end of the card have issues; more on that later).</p>

<p>With low diversity, the neural network generated cards that are oddly biased toward card names which include the letter &ldquo;S&rdquo;. The card text also conforms to the rules of the game very well.</p>
<div class="highlight"><pre><code class="language-" data-lang="">[Reality Spider@{3}{G}#Creature — Elf Warrior$Whenever ~ deals combat damage to a player, put a +1/+1 counter on it.%^]
[Dark Soul@{2}{R}#Instant$~ deals 2 damage to each creature without flying.%^]
[Standing Stand@{2}{G}#Creature — Elf Shaman${1}{G}, {T}: Draw a card, then discard a card.%^]
</code></pre></div>
<p>In contrast, cards generated with high diversity hit the uncanny valley of coherence and incoherence in both text and game mechanic abuse, which is what makes them interesting. </p>
<div class="highlight"><pre><code class="language-" data-lang="">[Portrenline@{2}{R}#Sorcery$As an additional cost to cast ~, exile ~.%^]
[Clocidian Lorid@{W}{W}{W}#Instant$Regenerate each creature with flying and each player.%^]
[Icomic Convermant@{3}{G}#Sorcery$Search your library for a land card in your graveyard.%1^1]
</code></pre></div>
<p>The best-of-both-worlds cards are generated from diversity parameters between both extremes, and often have funny names.</p>
<div class="highlight"><pre><code class="language-" data-lang="">[Seal Charm@{W}{W}#Instant$Exile target creature. Its controller loses 1 life.%^]
[Shambling Assemblaster@{4}{W}#Creature — Human Cleric$When ~ enters the battlefield, destroy target nonblack creature.%1^1]
[Lightning Strength@{3}{R}#Enchantment — Aura$Enchant creature|Enchanted creature gets +3/+3 and has flying, flying, trample, trample, lifelink, protection from black and votile all damage unless you return that card to its owner's hand.%2^2]
[Skysor of Shadows@{7}{B}{B}{B}#Enchantment$As ~ enters the battlefield, choose one —|• Put a -1/-1 counter on target creature.%2^2]
[Glinding Stadiers@{4}{W}#Creature — Spirit$Protection from no creatures can't attack.%^]
[Dragon Gault@{3}{G}{U}{U}#Creature — Kraven$~'s power and toughness are 2.%2^2]
</code></pre></div>
<p>All Keras/Python code used in this blog post, along with sample Magic card output and the trained model itself, is available open-source <a href="https://github.com/minimaxir/char-embeddings">in this GitHub repository</a>. The repo additionally contains <a href="https://github.com/minimaxir/char-embeddings/blob/master/text_generator_keras_sample.py">a Python script</a> which lets you generate new cards using the model, too!</p>

<h2>Visualizing Model Performance</h2>

<p>One thing deep learning tutorials rarely mention is <em>how</em> to collect the loss data and visualize the change in loss over time. Thanks to Keras&rsquo;s <a href="https://keras.io/callbacks/">utility functions</a>, I wrote a custom model callback which collects the batch losses and epoch losses and writes them to a CSV file.</p>

<p>Using R and ggplot2, I can plot the batch loss at every 50th batch to visualize how the model converges over time.</p>

<p><img src="/img/char-embeddings/batch-losses.png" alt=""></p>

<p>After 20 epochs, the model loss ends up at about <strong>0.30</strong> which is more-than-low-enough for coherent text. As you can see, there are large diminishing returns after a few epochs, which is the hard part of training deep learning models.</p>

<p>Plotting the epoch loss over the batches makes the trend more clear.</p>

<p><img src="/img/char-embeddings/epoch-losses.png" alt=""></p>

<p>In order to prevent early convergence, we can make the model more complex (i.e. stack more layers unironically), but that has trade-offs, both in training <em>and</em> predictive speed, the latter of which is important if using deep learning in a production application.</p>

<p>Lastly, as with the Google One Billion Words benchmark, we can extract the <a href="https://github.com/minimaxir/char-embeddings/blob/master/output/char-embeddings.txt">trained character embeddings</a> from the model (now augmented with Magic card context!) and plot them again to see what has changed.</p>

<p><img src="/img/char-embeddings/char-tsne-embed.png" alt=""></p>

<p>There are more pairs of uppercase/lowercase characters, although  interestingly there isn&rsquo;t much grouping with the special characters added as section breaks in the encoding, or mechanical uppercase characters such as W/U/B/R/G/C/T.</p>

<h2>Next Steps</h2>

<p>After building the model, I did a little more research to see if others solved the power/toughness problem. Since the sentences are only 40 characters and Magic cards are much longer than 40 characters, it&rsquo;s likely that power/toughness are out-of-scope for the model and it cannot learn their exact values. Turns out that the intended solution is to use a <a href="https://github.com/billzorn/mtgencode">completely different encoding</a>, such as this one for Dragon Whelp:</p>
<div class="highlight"><pre><code class="language-" data-lang="">|5creature|4|6dragon|7|8&amp;^^/&amp;^^^|9flying\{RR}: @ gets +&amp;^/+&amp; until end of turn. if this ability has been activated four or more times this turn, sacrifice @ at the beginning of the next end step.|3{^^RRRR}|0N|1dragon whelp|
</code></pre></div>
<p>Power/toughness are generated near the <em>beginning</em> of the card. Sections are delimited by pipes, with a numeral designating the corresponding section. Instead of numerals being used card values, carets are used, which provides a more accurate <em>quantification</em> of values. With this encoding, each character has a <em>singular purpose</em> in the global card context, and their embeddings would likely generate more informative visualizations. (But as a consequence, the generated cards are harder to parse at a glance).</p>

<p>The secondary encoding highlights a potential flaw in my methodology using pretrained character embeddings. Trained machine learning models must be used apples-to-apples on similar datasets; for example, you can&rsquo;t accurately perform Twitter <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> on a dataset using a model trained on professional movie reviews since Tweets do not follow <a href="https://owl.english.purdue.edu/owl/resource/735/02/">AP Style</a> guidelines. In my case, the <a href="http://commoncrawl.org">Common Crawl</a>, the source of the pretrained embeddings, follows more natural text usage and would not work analogously with the atypical character usages in <em>either</em> of the Magic card encodings.</p>

<p>There&rsquo;s still a <em>lot</em> of work to be done in terms of working with both pretrained character embeddings and improving Magic card generation, but I believe there is promise. The better way to make character embeddings than my script is to do it the hard way and train then manually, maybe even at a higher dimensionality like 500D or 1000D. Likewise, for Magic model building, the <a href="https://github.com/billzorn/mtgencode#training-a-neural-net">mtg-rnn instructions</a> repo uses a large LSTM stacked on a LSTM along with 120/200-character sentences, both of which combined make training <strong>VERY</strong> slow (notably, this was the architecture of the <a href="https://github.com/fchollet/keras/commit/d2b229df2ea0bab712379c418115bc44508bc6f9#diff-904d72bcf9fa38b32f9c1f868ff59367">very first commit</a> for the Keras text generation example, and <a href="https://github.com/fchollet/keras/commit/01d5e7bc4782daafcfa99e035c1bdbe13a985145">was changed</a> to the easily-trainable architecture). There is also promise in a <a href="http://kvfrans.com/variational-autoencoders-explained/">variational autoencoder</a> approach, such as with <a href="https://arxiv.org/abs/1702.02390">textvae</a>.</p>

<p>This work is potentially very expensive and I am strongly considering setting up a <a href="https://www.patreon.com">Patreon</a> in lieu of excess venture capital to subsidize my machine learning/deep learning tasks in the future.</p>

<p>At minimum, working with this example gave me a sufficient application of practical work with Keras, and another tool in my toolbox for data analysis and visualization. Keras makes the model-construction aspect of deep learning trivial and not scary. Hopefully, this article justifies the use of the &ldquo;deep learning&rdquo; buzzword in the headline.</p>

<p>It&rsquo;s also worth mentioning that I actually started working on automatic text generation 6 months ago using a different, non-deep-learning approach, but hit a snag and abandoned that project. With my work on Keras, I found a way around that snag, and on the same Magic dataset with the same input construction, I obtained a model loss of <strong>0.03</strong> at <strong>20% of the cloud computing cost</strong> in about the same amount of time. More on that later.</p>

<hr>

<p><em>The code for generating the R/ggplot2 data visualizations is available in this <a href="http://minimaxir.com/notebooks/char-tsne/">R Notebook</a>, and open-sourced in <a href="https://github.com/minimaxir/char-tsne-visualization">this GitHub Repository.</a></em></p>

<p><em>You are free to use the automatic text generation scripts and data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Predicting And Mapping Arrest Types in San Francisco with LightGBM, R, ggplot2]]></title>
    <link href="http://minimaxir.com/2017/02/predicting-arrests/"/>
    <updated>2017-02-08T06:30:00-08:00</updated>
    <id>http://minimaxir.com/2017/02/predicting-arrests</id>
    <content type="html"><![CDATA[<p>The new hotness in the world of data science is <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural networks</a>, which form the basis of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. But while everyone is obsessing about neural networks and how deep learning is <em>magic</em> and can solve <em>any</em> problem if you just <a href="https://www.reddit.com/r/ProgrammerHumor/comments/5si1f0/machine_learning_approaches/">stack enough layers</a>, there have been many recent developments in the relatively nonmagical world of machine learning with <em>boring</em> CPUs.</p>

<p>Years before neural networks were the Swiss army knife of data science, there were <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient-boosted machines</a>/<a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting">gradient-boosted trees</a>. GBMs/GBTs are machine learning methods which are effective on many types of data, and do not require the <a href="http://r-statistics.co/Assumptions-of-Linear-Regression.html">traditional model assumptions</a> of linear/logistic regression models. Wikipedia has a good article on the advantages of <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree learning</a>, and visual diagrams of the architecture:</p>

<p><img src="/img/predicting-arrests/CART_tree_titanic_survivors.png" alt=""></p>

<p>GBMs, as <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">implemented</a> in the Python package <a href="http://scikit-learn.org/stable/">scikit-learn</a>, are extremely popular in <a href="https://www.kaggle.com">Kaggle</a> machine learning competitions. But scikit-learn is relatively old, and new technologies have emerged which implement GBMs/GBTs on large datasets with massive parallelization and and in-memory computation. A popular big data machine learning library, <a href="http://www.h2o.ai">H2O</a>, has a <a href="http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/gbm-randomforest/index.html">famous GBM implementation</a> which, <a href="https://github.com/szilard/benchm-ml">per benchmarks</a>, is over 10x faster than scikit-learn and is optimized for datasets with millions of records. But even <em>faster</em> than H2O is <a href="https://github.com/dmlc/xgboost">xgboost</a>, which can hit a 5x-10x speed-ups relative to H2O, depending on the dataset size.</p>

<p>Enter <a href="https://github.com/Microsoft/LightGBM">LightGBM</a>, a new (October 2016) open-source machine learning framework by <a href="https://www.microsoft.com/en-us/">Microsoft</a> which, per <a href="https://github.com/Microsoft/LightGBM/issues/211">benchmarks</a> on release, was up to <em>4x faster</em> than xgboost! (xgboost very recently implemented a <a href="https://github.com/dmlc/xgboost/issues/1950">technique</a> also used in LightGBM, which reduced the relative speedup to just ~2x). As a result, LightGBM allows for very efficient model building on large datasets without requiring cloud computing or nVidia CUDA GPUs.</p>

<p>A year ago, I <a href="http://minimaxir.com/2015/12/sf-arrests/">wrote an analysis</a> of the types of police arrests in San Francisco, using data from the <a href="https://data.sfgov.org">SF OpenData</a> initiative, with a <a href="http://minimaxir.com/2015/12/sf-arrest-maps/">followup article</a> analyzing the locations of these arrests. Months later, the same source dataset was used <a href="https://www.kaggle.com/c/sf-crime">for a Kaggle competition</a>. Why not give the dataset another look and test LightGBM out?</p>

<h2>Playing With The Data</h2>

<p><em>(You can view the R code used to process the data and generate the data visualizations in <a href="http://minimaxir.com/notebooks/predicting-arrests/">this R Notebook</a>)</em></p>

<p>The <a href="https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry">SFPD Incidents</a> dataset includes crime incidents in San Francisco from 1/1/2003 to 1/17/2017 (at time of analysis). Filtering the dataset only on incidents which resulted in arrests (since most incidents are trivial) leaves a dataset of 634,299 arrests total. The dataset also includes information on the type of crime, the location where the arrest occurred, and the date/time. There are 39 different types of arrests in the <strong>Category</strong> column such as Assault, Burglary, and Prostitution, which serves as the response variable.</p>

<p><img src="/img/predicting-arrests/data.png" alt=""></p>

<p>Meanwhile, we can engineer features from the location and date/time. 
Performing an exploratory data analysis of both is helpful to determine at a glance which features may be relevant (fortunately, I did that a year ago).</p>

<p><img src="/img/sf-arrests/sf-arrest-when-4.png" alt=""></p>

<p>The location is given as latitude/longitude coordinates, so we can select a longitude <strong>X</strong> and latitude <strong>Y</strong> as features. Date/Time can be deconstructed further. We can extract the <strong>hour</strong> in which a given arrest occurred as a feature (hour can take 24 different values from 0 — 23). Likewise, we can extract the <strong>month</strong> in a similar manner (12 values, from 1 — 12). The <strong>year</strong> the crime occurred can be extracted without special encoding. (2003 — 2017). It is always helpful to include a year feature in predictive models to help account for change over time. The <strong>DayOfWeek</strong> is important, but encoding it as a numeric value is tricker; we logically encode each day of the week from 1 — 7, but which day should be #1? Making Monday #1 and Sunday #7 is the most logical, since a decision tree rule that sets a threshold on DayOfWeek values &gt; 5 will translate logically to a weekend.</p>

<p><img src="/img/predicting-arrests/predict_matrix.png" alt=""></p>

<p>That&rsquo;s six features total. There are more features which could be helpful, but let&rsquo;s check a baseline model as a start.</p>

<h2>Modeling</h2>

<p>Specifically, the model will predict the answer the question: <em>given that a San Francisco police arrest occurs at a specified time and place, what is the reason for that arrest?</em></p>

<p>For this post, I will use the <a href="https://github.com/Microsoft/LightGBM/tree/master/R-package">R package</a> for LightGBM (which was beta-released in January 2017; it&rsquo;s <em>extremely</em> cutting edge!) We split the dataset 70%/30% into a training set of 444,011 arrests and a test set of 190,288 arrests (due to the large amount of different category labels, the split must be <a href="https://en.wikipedia.org/wiki/Stratified_sampling">stratified</a> to ensure the training and test sets have a balanced distribution of labels; in R, this can be implemented with the <code>caret</code> package and <code>createDataPartition</code>).</p>

<p>LightGBM trains the model on the training set and evaluates it on the test set to minimize the <a href="https://www.kaggle.com/c/sf-crime#evaluation">multiclass logarithmic loss</a> of the model. For now, I use the <a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md">default parameters</a> of LightGBM, except to massively increase the number of iterations of the training algorithm, and to stop training the model early if the model stops improving. After about 4 minutes on my laptop (which is very fast for a dataset of this size!), the model returns a multilogloss of <strong>1.98</strong>.</p>

<p>That number sounds arbitrary. Is it good or bad? Let&rsquo;s compare it to the multilogloss from the <a href="https://www.kaggle.com/c/sf-crime/leaderboard">top models</a> from the Kaggle version of the dataset, where a lower score is better:</p>

<p><img src="/img/predicting-arrests/kaggle.png" alt=""></p>

<p>&hellip;okay, 1.98 <em>is</em> a good score, and without spending much time adding features to the model and <a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md">tuning parameters</a>! To be fair, my methodology would not necessarily result in the same score on the Kaggle dataset, but it confirms that the LightGBM model is in the top tier of models available for this problem and dataset context. And it didn&rsquo;t <a href="https://www.kaggle.com/smerity/sf-crime/fighting-crime-with-keras/output">require any neural networks</a> either!</p>

<p>There are areas for improvement in feature engineering which <a href="https://www.kaggle.com/c/sf-crime/kernels">other entries</a> in the Kaggle competition implemented, such as a <a href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)">dummy variable</a> indicating whether the offense occurred at an intersection and which SF police station was involved in the arrest. We could also encode features such as hour and DayOfWeek as categorical features (LightGBM conveniently allows this without requiring <a href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a> the features) instead of numeric, but in my brief testing, it made the model <em>worse</em>, interestingly.</p>

<h2>Analyzing the LightGBM Model</h2>

<p>Another perk of not using a neural network for statistical model building is the ability to learn more about the importance of features in a model, as opposed to it being a <a href="https://en.wikipedia.org/wiki/Black_box">black box</a>. In the case of gradient boosting, we can calculate the proportional contribution of each feature to the total <a href="https://en.wikipedia.org/wiki/Information_gain_in_decision_trees">information gain</a> of the model, which will help identify the most important features, and potentially unhelpful features:</p>

<p><img src="/img/predicting-arrests/imp.png" alt=""></p>

<p>Unsurprisingly, location features are the most important, with both location-based features establishing 70% of the total Gain in the model. But no feature is completely insignificant, which is a good thing.</p>

<p>Back to the multilogloss of 1.98. What does that mean in the real world? What is the <em>accuracy</em> of the model? We run each of the 190,288 arrests in the test set against the model, which returns 39 probability values for each record: one for each possible category of arrest. The category with the highest probability becomes the <strong>predicted</strong> type of arrest.</p>

<p><img src="/img/predicting-arrests/predicted_results.png" alt=""></p>

<p>The accuracy of the model on the test set, which is the proportion of predictions where the predicted category value matches the <strong>actual</strong> category value, is <strong>39.7%</strong>, with a 95% confidence interval for the true accuracy between 39.5% and 39.9%. That seems low! However, there is catch-all &ldquo;Other Offenses&rdquo; category for an arrest; if you predicted a &ldquo;Other Offenses&rdquo; label for all the test-set values, you would get an accuracy of <em>31.1%</em>, which serves as the No Information Rate (since it would be the highest accuracy approach if there was no information at all). A 8.6 percentage point improvement is still an improvement though; many industries would <em>love</em> an 8.6 percentage point increase in accuracy, but for this context obviously it&rsquo;s not enough to usher in a <a href="https://en.wikipedia.org/wiki/Minority_Report_(film)">Minority Report</a>/<a href="https://en.wikipedia.org/wiki/Person_of_Interest_(TV_series)">Person of Interest</a> future.</p>

<p>We can visualize the classifications on the test set by the model using a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>; <code>caret</code> has a simple <code>confusionMatrix()</code> function, and ggplot2 has a <code>geom_tile()</code> to map out the relationships, even with 39 classes. We can also annotate the tiles where actual label = predicted label by drawing a <code>geom_point()</code> on top. Putting it all together:</p>

<p><img src="/img/predicting-arrests/confusionMatrix.png" alt=""></p>

<p>There is, indeed, a large amount of confusion. Many of the labels are mispredicted as Other Offenses. Specifically, the model frequently confuses the combinations of Assault, Drug/Narcotics, Larceny/Theft, and Warrants, suggesting that they also may be catch-alls.</p>

<p>In theory, the predicted probabilities from the model between similar types of crime should also be similar, which may be causing these mispredictions. We can calculate the <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlations</a> between the predicted probabilities, and use <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical clustering</a> to <a href="http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization">arrange and plot the correlations</a> and their labels in a logical order. The majority of the correlations between labels are between 0 and +/- 0.5 (weak to moderate), but their arrangement tells a different story:</p>

<p><img src="/img/predicting-arrests/correlationMatrix.png" alt=""></p>

<p>From top to bottom, you can see that there is a grouping of more blue-collar, physical crimes types (Assault, Vandalism), then a grouping of less-physical, white-collar crime types (Bribery, Extortion), and then a smaller grouping of seedier crime types (Liquor Laws, Prostitution).</p>

<p>The visualization doesn&rsquo;t necessarily provide more information about the confusion matrix and the mispredictions, but <em>it looks cool</em>, which is enough.</p>

<h2>Mapping the Predicted Types of Arrests</h2>

<p>Kaggle competitions emphasize model creation, but don&rsquo;t discuss how to implement and execute models in practice. Since we can predict the type of crime based on the given location and date/time of an arrest, we can map boundaries of the mostly likely type of offense. Using <code>ggmap</code> to get a map of San Francisco, splitting San Francisco into tens of thousands of points, and predicting the most-likely type of arrest at the location with a given date/time.</p>

<p>Let&rsquo;s say we want to predict the types of crime in the future, on April 15th, 2017, during 8 PM. We construct a dataset of those points and the same date/time features used to generate the model originally. Then run those fabricated points through the model again to get new predicted labels (Additionally, we need to remove &ldquo;Other Offenses&rdquo; predicted labels since they cloud up the map). Plotting each point as a <code>geom_tile</code> will interpolate regions around the city. Putting it all together:</p>

<p><img src="/img/predicting-arrests/crime-2017-04-15-20.png" alt=""></p>

<p>Not too shabby. But that&rsquo;s not all; we can <em>animate</em> this map over a day by incrementing the hour, generating a map for each hour (while keeping the colors corresponding to the arrest type consistent), and then <a href="https://github.com/minimaxir/frames-to-gif-osx">stitching the maps together</a> into a GIF. Let&rsquo;s do March 14th, 2017 (<a href="https://en.wikipedia.org/wiki/Pi_Day">Pi Day</a> can be dangerous!) starting at 6 AM:</p>

<p><img src="/img/predicting-arrests/map_ani.gif" alt=""></p>

<p>Wow!</p>

<h2>Conclusion</h2>

<p>I deliberately avoided using the term &ldquo;machine learning&rdquo; in the headline of this post because it has been overused to the point of clickbait. Indeed, neural networks/deep learning excel at processing higher-dimensional data such as text, image, and voice data, but in cases where dataset features are <a href="https://news.ycombinator.com/item?id=13563892">simple and known</a>, neural networks are not necessarily the most <em>pragmatic</em> option. CPU/RAM machine learning libraries like LightGBM are still worthwhile, despite the religious fervor for deep learning.</p>

<p>And there&rsquo;s still a lot of work that can be done with the SF Crime Incidents dataset. The model only predicts the type of crime given an arrest occurred; it does not predict <em>if</em> an arrest will occur at a given time and place, which would make a fun project for the future!</p>

<hr>

<p><em>You can view all the R and ggplot2 code used to visualize the San Francisco crime data in <a href="http://minimaxir.com/notebooks/predicting-arrests/">this R Notebook</a>. You can also view the images/data used for this post in <a href="https://github.com/minimaxir/sf-arrests-predict">this GitHub repository</a></em>.</p>

<p><em>You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p>
]]></content>
  </entry>
  
</feed>
